{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HereHackathon.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3EwKLySpCo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Description: This program calculates Liveability Factor as good or bad\n",
        "#                       using Artificial Neural Networks (ANN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv3v1F3xrcFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "662ffdef-c468-4ed3-eeef-2ce6ee5fe540"
      },
      "source": [
        "#import libraries\n",
        "import glob\n",
        "from keras.models import Sequential, load_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import keras as k\n",
        "import datetime"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6cax23TrfWv",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "54bf1ff1-954b-45d0-e88f-b5201e925e57"
      },
      "source": [
        "#Load the data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-72320c97-4feb-4f05-8d17-1358c3b62301\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-72320c97-4feb-4f05-8d17-1358c3b62301\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving liveability_dataset.csv to liveability_dataset.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmTjwFKDr06b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "972d7aa6-d7db-493f-c292-fe57bc638ab7"
      },
      "source": [
        "df = pd.read_csv('liveability_dataset.csv')\n",
        "\n",
        "#Print the first 5 rows\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>age</th>\n",
              "      <th>environment</th>\n",
              "      <th>sg</th>\n",
              "      <th>rating</th>\n",
              "      <th>ft</th>\n",
              "      <th>use</th>\n",
              "      <th>puse</th>\n",
              "      <th>pcc</th>\n",
              "      <th>bt</th>\n",
              "      <th>bgr</th>\n",
              "      <th>ta</th>\n",
              "      <th>crime</th>\n",
              "      <th>traffic</th>\n",
              "      <th>hg</th>\n",
              "      <th>publicspots</th>\n",
              "      <th>pft</th>\n",
              "      <th>distance</th>\n",
              "      <th>food</th>\n",
              "      <th>infrastructure</th>\n",
              "      <th>dm</th>\n",
              "      <th>cad</th>\n",
              "      <th>perf</th>\n",
              "      <th>pe</th>\n",
              "      <th>ane</th>\n",
              "      <th>classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>121.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.4</td>\n",
              "      <td>44</td>\n",
              "      <td>7800</td>\n",
              "      <td>5.2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.3</td>\n",
              "      <td>38</td>\n",
              "      <td>6000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>62.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>423.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.6</td>\n",
              "      <td>31</td>\n",
              "      <td>7500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>48.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1.005</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>present</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>117.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>111.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>11.2</td>\n",
              "      <td>32</td>\n",
              "      <td>6700</td>\n",
              "      <td>3.9</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>51.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>106.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.6</td>\n",
              "      <td>35</td>\n",
              "      <td>7300</td>\n",
              "      <td>4.6</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  age   environment     sg  rating  ...  cad  perf   pe  ane classification\n",
              "0   0  48.0         80.0  1.020     1.0  ...   no  good   no   no            ckd\n",
              "1   1   7.0         50.0  1.020     4.0  ...   no  good   no   no            ckd\n",
              "2   2  62.0         80.0  1.010     2.0  ...   no  poor   no  yes            ckd\n",
              "3   3  48.0         70.0  1.005     4.0  ...   no  poor  yes  yes            ckd\n",
              "4   4  51.0         80.0  1.010     2.0  ...   no  good   no   no            ckd\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zehYOSur6cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f48bcef0-f72e-4814-f6e8-c2654d1f7ff4"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 26)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHjwH56Or9mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating a list of column names to keep\n",
        "columns_to_retain = ['environment','rating','crime','traffic','publicspots','distance','food','infrastructure','classification']\n",
        "\n",
        "#Drop the columns that are not wanted in columns_to_retain\n",
        "df = df.drop( [col for col in df.columns if not col in columns_to_retain] , axis=1 )\n",
        "\n",
        "#Drop the rows with na or missing values\n",
        "df = df.dropna(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QUHaaEbsUTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Transform the non-numeric data in the columns\n",
        "for column in df.columns:\n",
        "  if df[column].dtype == np.number:\n",
        "    continue\n",
        "  df[column] = LabelEncoder().fit_transform( df[column] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5TQOQNusW4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eca929b8-cee8-45af-f9bd-d350b91c3375"
      },
      "source": [
        "#print the first 5 rows of the new cleaned data set\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>environment</th>\n",
              "      <th>rating</th>\n",
              "      <th>crime</th>\n",
              "      <th>traffic</th>\n",
              "      <th>publicspots</th>\n",
              "      <th>distance</th>\n",
              "      <th>food</th>\n",
              "      <th>infrastructure</th>\n",
              "      <th>classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>111.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>49</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>90.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>142.0</td>\n",
              "      <td>12.2</td>\n",
              "      <td>59</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>90.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.2</td>\n",
              "      <td>114.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>70.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>131.0</td>\n",
              "      <td>10.8</td>\n",
              "      <td>32</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>70.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>138.0</td>\n",
              "      <td>9.7</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    environment  rating  crime  ...  food  infrastructure  classification\n",
              "3          70.0     4.0    3.8  ...    15               1               0\n",
              "5          90.0     3.0    1.1  ...    20               1               0\n",
              "9          90.0     2.0    7.2  ...    13               1               0\n",
              "11         70.0     3.0    2.7  ...    14               1               0\n",
              "12         70.0     3.0    2.1  ...    10               1               0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q5lYhpTsZwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data into independent (X) data set -features  and (Y) data set -targets\n",
        "X = df.drop(['classification'], axis=1)\n",
        "Y = df['classification']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgTi1YqAscqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Scaling\n",
        "#min-max scaler method scales the data set so that all the input features lie between 0 and 1\n",
        "X_scaler = MinMaxScaler()\n",
        "X_scaler.fit(X)\n",
        "column_names = X.columns\n",
        "X[column_names] = X_scaler.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOZ9lA7-sfNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data into 80% training and 20% testing and Shuffle \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvcvZFnhsiVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build the model\n",
        "model = Sequential()\n",
        "model.add( Dense(256, input_dim= len(X.columns) , kernel_initializer= k.initializers.random_normal(seed=13), activation='relu') )\n",
        "model.add( Dense(1, activation='hard_sigmoid') )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMJfXhs3slnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "a814b80c-12a3-43d5-8729-210a191e0cf0"
      },
      "source": [
        "#Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ezEOkVWsqek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfb23003-b270-449d-8dbc-89e3a78c5025"
      },
      "source": [
        "#Train the model\n",
        "history = model.fit(X_train, Y_train, epochs = 1000, batch_size= X_train.shape[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "168/168 [==============================] - 1s 3ms/step - loss: 0.6990 - acc: 0.4524\n",
            "Epoch 2/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.6919 - acc: 0.6131\n",
            "Epoch 3/1000\n",
            "168/168 [==============================] - 0s 40us/step - loss: 0.6839 - acc: 0.6548\n",
            "Epoch 4/1000\n",
            "168/168 [==============================] - 0s 25us/step - loss: 0.6756 - acc: 0.6548\n",
            "Epoch 5/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.6672 - acc: 0.6786\n",
            "Epoch 6/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.6588 - acc: 0.7083\n",
            "Epoch 7/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.6505 - acc: 0.7321\n",
            "Epoch 8/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.6422 - acc: 0.7440\n",
            "Epoch 9/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.6342 - acc: 0.7560\n",
            "Epoch 10/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.6263 - acc: 0.7679\n",
            "Epoch 11/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.6185 - acc: 0.7798\n",
            "Epoch 12/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.6108 - acc: 0.7857\n",
            "Epoch 13/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.6031 - acc: 0.7857\n",
            "Epoch 14/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.5955 - acc: 0.7917\n",
            "Epoch 15/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.5879 - acc: 0.7976\n",
            "Epoch 16/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.5805 - acc: 0.7976\n",
            "Epoch 17/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.5731 - acc: 0.8036\n",
            "Epoch 18/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.5659 - acc: 0.8155\n",
            "Epoch 19/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.5588 - acc: 0.8155\n",
            "Epoch 20/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.5516 - acc: 0.8274\n",
            "Epoch 21/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.5445 - acc: 0.8333\n",
            "Epoch 22/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.5373 - acc: 0.8393\n",
            "Epoch 23/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.5302 - acc: 0.8393\n",
            "Epoch 24/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.5229 - acc: 0.8393\n",
            "Epoch 25/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.5157 - acc: 0.8512\n",
            "Epoch 26/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.5084 - acc: 0.8571\n",
            "Epoch 27/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.5010 - acc: 0.8631\n",
            "Epoch 28/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.4936 - acc: 0.8690\n",
            "Epoch 29/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.4862 - acc: 0.8690\n",
            "Epoch 30/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.4787 - acc: 0.8750\n",
            "Epoch 31/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.4712 - acc: 0.8810\n",
            "Epoch 32/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.4635 - acc: 0.8810\n",
            "Epoch 33/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.4558 - acc: 0.8869\n",
            "Epoch 34/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.4481 - acc: 0.8869\n",
            "Epoch 35/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.4403 - acc: 0.8869\n",
            "Epoch 36/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.4324 - acc: 0.8869\n",
            "Epoch 37/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.4244 - acc: 0.8869\n",
            "Epoch 38/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.4165 - acc: 0.8869\n",
            "Epoch 39/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.4084 - acc: 0.8988\n",
            "Epoch 40/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.4003 - acc: 0.8988\n",
            "Epoch 41/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.3921 - acc: 0.8988\n",
            "Epoch 42/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.3839 - acc: 0.8988\n",
            "Epoch 43/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.3755 - acc: 0.9048\n",
            "Epoch 44/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.3672 - acc: 0.9048\n",
            "Epoch 45/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.3587 - acc: 0.9048\n",
            "Epoch 46/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.3502 - acc: 0.9107\n",
            "Epoch 47/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.3417 - acc: 0.9107\n",
            "Epoch 48/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.3330 - acc: 0.9107\n",
            "Epoch 49/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.3244 - acc: 0.9107\n",
            "Epoch 50/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.3157 - acc: 0.9107\n",
            "Epoch 51/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.3070 - acc: 0.9107\n",
            "Epoch 52/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.2983 - acc: 0.9107\n",
            "Epoch 53/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.2897 - acc: 0.9107\n",
            "Epoch 54/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.2812 - acc: 0.9107\n",
            "Epoch 55/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.2727 - acc: 0.9107\n",
            "Epoch 56/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.2642 - acc: 0.9167\n",
            "Epoch 57/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.2559 - acc: 0.9226\n",
            "Epoch 58/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.2477 - acc: 0.9226\n",
            "Epoch 59/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.2396 - acc: 0.9226\n",
            "Epoch 60/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.2316 - acc: 0.9226\n",
            "Epoch 61/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.2239 - acc: 0.9226\n",
            "Epoch 62/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.2164 - acc: 0.9226\n",
            "Epoch 63/1000\n",
            "168/168 [==============================] - 0s 22us/step - loss: 0.2095 - acc: 0.9226\n",
            "Epoch 64/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.2029 - acc: 0.9226\n",
            "Epoch 65/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.1965 - acc: 0.9226\n",
            "Epoch 66/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.1904 - acc: 0.9226\n",
            "Epoch 67/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.1847 - acc: 0.9226\n",
            "Epoch 68/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.1791 - acc: 0.9226\n",
            "Epoch 69/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.1739 - acc: 0.9286\n",
            "Epoch 70/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.1687 - acc: 0.9345\n",
            "Epoch 71/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.1637 - acc: 0.9345\n",
            "Epoch 72/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.1590 - acc: 0.9345\n",
            "Epoch 73/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.1547 - acc: 0.9345\n",
            "Epoch 74/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.1506 - acc: 0.9405\n",
            "Epoch 75/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.1468 - acc: 0.9464\n",
            "Epoch 76/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.1432 - acc: 0.9464\n",
            "Epoch 77/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.1397 - acc: 0.9464\n",
            "Epoch 78/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1363 - acc: 0.9464\n",
            "Epoch 79/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.1330 - acc: 0.9464\n",
            "Epoch 80/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1298 - acc: 0.9464\n",
            "Epoch 81/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1268 - acc: 0.9464\n",
            "Epoch 82/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.1240 - acc: 0.9464\n",
            "Epoch 83/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1213 - acc: 0.9464\n",
            "Epoch 84/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1187 - acc: 0.9464\n",
            "Epoch 85/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.1163 - acc: 0.9464\n",
            "Epoch 86/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.1140 - acc: 0.9464\n",
            "Epoch 87/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1119 - acc: 0.9464\n",
            "Epoch 88/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1099 - acc: 0.9464\n",
            "Epoch 89/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.1079 - acc: 0.9464\n",
            "Epoch 90/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1061 - acc: 0.9464\n",
            "Epoch 91/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.1044 - acc: 0.9464\n",
            "Epoch 92/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.1028 - acc: 0.9464\n",
            "Epoch 93/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.1013 - acc: 0.9464\n",
            "Epoch 94/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0999 - acc: 0.9464\n",
            "Epoch 95/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0985 - acc: 0.9464\n",
            "Epoch 96/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0971 - acc: 0.9464\n",
            "Epoch 97/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0959 - acc: 0.9464\n",
            "Epoch 98/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0948 - acc: 0.9464\n",
            "Epoch 99/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0937 - acc: 0.9464\n",
            "Epoch 100/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0927 - acc: 0.9464\n",
            "Epoch 101/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0917 - acc: 0.9464\n",
            "Epoch 102/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0908 - acc: 0.9464\n",
            "Epoch 103/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0898 - acc: 0.9464\n",
            "Epoch 104/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0890 - acc: 0.9524\n",
            "Epoch 105/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0881 - acc: 0.9524\n",
            "Epoch 106/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0873 - acc: 0.9524\n",
            "Epoch 107/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0865 - acc: 0.9524\n",
            "Epoch 108/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0858 - acc: 0.9524\n",
            "Epoch 109/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0851 - acc: 0.9524\n",
            "Epoch 110/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0844 - acc: 0.9583\n",
            "Epoch 111/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0838 - acc: 0.9583\n",
            "Epoch 112/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0831 - acc: 0.9583\n",
            "Epoch 113/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0826 - acc: 0.9583\n",
            "Epoch 114/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0820 - acc: 0.9583\n",
            "Epoch 115/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0816 - acc: 0.9583\n",
            "Epoch 116/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0811 - acc: 0.9583\n",
            "Epoch 117/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0806 - acc: 0.9583\n",
            "Epoch 118/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0802 - acc: 0.9583\n",
            "Epoch 119/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0798 - acc: 0.9583\n",
            "Epoch 120/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0793 - acc: 0.9583\n",
            "Epoch 121/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0789 - acc: 0.9583\n",
            "Epoch 122/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0785 - acc: 0.9643\n",
            "Epoch 123/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0781 - acc: 0.9643\n",
            "Epoch 124/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0777 - acc: 0.9643\n",
            "Epoch 125/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0773 - acc: 0.9643\n",
            "Epoch 126/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0769 - acc: 0.9643\n",
            "Epoch 127/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0765 - acc: 0.9643\n",
            "Epoch 128/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0761 - acc: 0.9702\n",
            "Epoch 129/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0758 - acc: 0.9702\n",
            "Epoch 130/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0754 - acc: 0.9702\n",
            "Epoch 131/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0750 - acc: 0.9762\n",
            "Epoch 132/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0747 - acc: 0.9762\n",
            "Epoch 133/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0743 - acc: 0.9762\n",
            "Epoch 134/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0740 - acc: 0.9762\n",
            "Epoch 135/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0737 - acc: 0.9762\n",
            "Epoch 136/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0733 - acc: 0.9762\n",
            "Epoch 137/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0730 - acc: 0.9762\n",
            "Epoch 138/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0727 - acc: 0.9762\n",
            "Epoch 139/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0723 - acc: 0.9762\n",
            "Epoch 140/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0720 - acc: 0.9762\n",
            "Epoch 141/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0717 - acc: 0.9762\n",
            "Epoch 142/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0714 - acc: 0.9762\n",
            "Epoch 143/1000\n",
            "168/168 [==============================] - 0s 53us/step - loss: 0.0711 - acc: 0.9762\n",
            "Epoch 144/1000\n",
            "168/168 [==============================] - 0s 25us/step - loss: 0.0707 - acc: 0.9762\n",
            "Epoch 145/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0704 - acc: 0.9762\n",
            "Epoch 146/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0701 - acc: 0.9762\n",
            "Epoch 147/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0698 - acc: 0.9821\n",
            "Epoch 148/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0695 - acc: 0.9821\n",
            "Epoch 149/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0692 - acc: 0.9821\n",
            "Epoch 150/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0689 - acc: 0.9821\n",
            "Epoch 151/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0685 - acc: 0.9821\n",
            "Epoch 152/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0682 - acc: 0.9821\n",
            "Epoch 153/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0679 - acc: 0.9821\n",
            "Epoch 154/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0676 - acc: 0.9821\n",
            "Epoch 155/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0673 - acc: 0.9821\n",
            "Epoch 156/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0670 - acc: 0.9821\n",
            "Epoch 157/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0667 - acc: 0.9821\n",
            "Epoch 158/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0664 - acc: 0.9821\n",
            "Epoch 159/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0661 - acc: 0.9821\n",
            "Epoch 160/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0658 - acc: 0.9821\n",
            "Epoch 161/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0655 - acc: 0.9821\n",
            "Epoch 162/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0652 - acc: 0.9821\n",
            "Epoch 163/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0649 - acc: 0.9821\n",
            "Epoch 164/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0646 - acc: 0.9821\n",
            "Epoch 165/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0643 - acc: 0.9821\n",
            "Epoch 166/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0641 - acc: 0.9821\n",
            "Epoch 167/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0638 - acc: 0.9821\n",
            "Epoch 168/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0635 - acc: 0.9821\n",
            "Epoch 169/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0632 - acc: 0.9821\n",
            "Epoch 170/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0629 - acc: 0.9821\n",
            "Epoch 171/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0626 - acc: 0.9821\n",
            "Epoch 172/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0624 - acc: 0.9821\n",
            "Epoch 173/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0621 - acc: 0.9821\n",
            "Epoch 174/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0618 - acc: 0.9821\n",
            "Epoch 175/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0615 - acc: 0.9821\n",
            "Epoch 176/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0613 - acc: 0.9821\n",
            "Epoch 177/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0610 - acc: 0.9821\n",
            "Epoch 178/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0607 - acc: 0.9821\n",
            "Epoch 179/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0605 - acc: 0.9821\n",
            "Epoch 180/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0602 - acc: 0.9821\n",
            "Epoch 181/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0600 - acc: 0.9821\n",
            "Epoch 182/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0597 - acc: 0.9821\n",
            "Epoch 183/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0595 - acc: 0.9821\n",
            "Epoch 184/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0592 - acc: 0.9821\n",
            "Epoch 185/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0590 - acc: 0.9821\n",
            "Epoch 186/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0588 - acc: 0.9821\n",
            "Epoch 187/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0586 - acc: 0.9821\n",
            "Epoch 188/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0584 - acc: 0.9821\n",
            "Epoch 189/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0582 - acc: 0.9821\n",
            "Epoch 190/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0580 - acc: 0.9821\n",
            "Epoch 191/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0577 - acc: 0.9821\n",
            "Epoch 192/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0575 - acc: 0.9821\n",
            "Epoch 193/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0573 - acc: 0.9821\n",
            "Epoch 194/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0571 - acc: 0.9821\n",
            "Epoch 195/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0569 - acc: 0.9821\n",
            "Epoch 196/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0567 - acc: 0.9821\n",
            "Epoch 197/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0565 - acc: 0.9821\n",
            "Epoch 198/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0563 - acc: 0.9821\n",
            "Epoch 199/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0561 - acc: 0.9821\n",
            "Epoch 200/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0559 - acc: 0.9821\n",
            "Epoch 201/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0557 - acc: 0.9821\n",
            "Epoch 202/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0555 - acc: 0.9821\n",
            "Epoch 203/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0553 - acc: 0.9821\n",
            "Epoch 204/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0551 - acc: 0.9821\n",
            "Epoch 205/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0549 - acc: 0.9821\n",
            "Epoch 206/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0547 - acc: 0.9821\n",
            "Epoch 207/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0545 - acc: 0.9821\n",
            "Epoch 208/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0543 - acc: 0.9821\n",
            "Epoch 209/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0541 - acc: 0.9821\n",
            "Epoch 210/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0539 - acc: 0.9821\n",
            "Epoch 211/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0537 - acc: 0.9821\n",
            "Epoch 212/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0535 - acc: 0.9821\n",
            "Epoch 213/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0533 - acc: 0.9821\n",
            "Epoch 214/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0531 - acc: 0.9821\n",
            "Epoch 215/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0529 - acc: 0.9821\n",
            "Epoch 216/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0527 - acc: 0.9821\n",
            "Epoch 217/1000\n",
            "168/168 [==============================] - 0s 29us/step - loss: 0.0525 - acc: 0.9821\n",
            "Epoch 218/1000\n",
            "168/168 [==============================] - 0s 30us/step - loss: 0.0523 - acc: 0.9821\n",
            "Epoch 219/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0521 - acc: 0.9821\n",
            "Epoch 220/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0519 - acc: 0.9821\n",
            "Epoch 221/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0517 - acc: 0.9821\n",
            "Epoch 222/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0515 - acc: 0.9821\n",
            "Epoch 223/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0513 - acc: 0.9821\n",
            "Epoch 224/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0511 - acc: 0.9821\n",
            "Epoch 225/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0509 - acc: 0.9821\n",
            "Epoch 226/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0507 - acc: 0.9821\n",
            "Epoch 227/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0505 - acc: 0.9821\n",
            "Epoch 228/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0503 - acc: 0.9821\n",
            "Epoch 229/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0501 - acc: 0.9821\n",
            "Epoch 230/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0499 - acc: 0.9821\n",
            "Epoch 231/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0497 - acc: 0.9821\n",
            "Epoch 232/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0495 - acc: 0.9821\n",
            "Epoch 233/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0493 - acc: 0.9821\n",
            "Epoch 234/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0492 - acc: 0.9821\n",
            "Epoch 235/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0490 - acc: 0.9821\n",
            "Epoch 236/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0488 - acc: 0.9821\n",
            "Epoch 237/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0486 - acc: 0.9821\n",
            "Epoch 238/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0485 - acc: 0.9821\n",
            "Epoch 239/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0483 - acc: 0.9821\n",
            "Epoch 240/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0481 - acc: 0.9821\n",
            "Epoch 241/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0480 - acc: 0.9821\n",
            "Epoch 242/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0478 - acc: 0.9821\n",
            "Epoch 243/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0476 - acc: 0.9821\n",
            "Epoch 244/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0474 - acc: 0.9821\n",
            "Epoch 245/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0473 - acc: 0.9821\n",
            "Epoch 246/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0471 - acc: 0.9821\n",
            "Epoch 247/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0470 - acc: 0.9821\n",
            "Epoch 248/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0468 - acc: 0.9821\n",
            "Epoch 249/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0467 - acc: 0.9821\n",
            "Epoch 250/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0465 - acc: 0.9821\n",
            "Epoch 251/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0464 - acc: 0.9821\n",
            "Epoch 252/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0462 - acc: 0.9821\n",
            "Epoch 253/1000\n",
            "168/168 [==============================] - 0s 10us/step - loss: 0.0461 - acc: 0.9821\n",
            "Epoch 254/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0459 - acc: 0.9821\n",
            "Epoch 255/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0458 - acc: 0.9821\n",
            "Epoch 256/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0457 - acc: 0.9821\n",
            "Epoch 257/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0455 - acc: 0.9821\n",
            "Epoch 258/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0454 - acc: 0.9821\n",
            "Epoch 259/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0452 - acc: 0.9821\n",
            "Epoch 260/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0451 - acc: 0.9821\n",
            "Epoch 261/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0450 - acc: 0.9821\n",
            "Epoch 262/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0448 - acc: 0.9821\n",
            "Epoch 263/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0447 - acc: 0.9821\n",
            "Epoch 264/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0445 - acc: 0.9821\n",
            "Epoch 265/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0444 - acc: 0.9821\n",
            "Epoch 266/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0443 - acc: 0.9821\n",
            "Epoch 267/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0441 - acc: 0.9821\n",
            "Epoch 268/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0440 - acc: 0.9821\n",
            "Epoch 269/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0439 - acc: 0.9821\n",
            "Epoch 270/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0437 - acc: 0.9821\n",
            "Epoch 271/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0436 - acc: 0.9821\n",
            "Epoch 272/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0434 - acc: 0.9821\n",
            "Epoch 273/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0433 - acc: 0.9821\n",
            "Epoch 274/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0432 - acc: 0.9821\n",
            "Epoch 275/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0430 - acc: 0.9821\n",
            "Epoch 276/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0429 - acc: 0.9821\n",
            "Epoch 277/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0428 - acc: 0.9821\n",
            "Epoch 278/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0426 - acc: 0.9821\n",
            "Epoch 279/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0425 - acc: 0.9821\n",
            "Epoch 280/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0423 - acc: 0.9821\n",
            "Epoch 281/1000\n",
            "168/168 [==============================] - 0s 10us/step - loss: 0.0422 - acc: 0.9821\n",
            "Epoch 282/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0421 - acc: 0.9821\n",
            "Epoch 283/1000\n",
            "168/168 [==============================] - 0s 10us/step - loss: 0.0419 - acc: 0.9821\n",
            "Epoch 284/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0418 - acc: 0.9821\n",
            "Epoch 285/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0417 - acc: 0.9821\n",
            "Epoch 286/1000\n",
            "168/168 [==============================] - 0s 10us/step - loss: 0.0415 - acc: 0.9821\n",
            "Epoch 287/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0414 - acc: 0.9821\n",
            "Epoch 288/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0412 - acc: 0.9821\n",
            "Epoch 289/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0411 - acc: 0.9821\n",
            "Epoch 290/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0410 - acc: 0.9821\n",
            "Epoch 291/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0408 - acc: 0.9821\n",
            "Epoch 292/1000\n",
            "168/168 [==============================] - 0s 9us/step - loss: 0.0407 - acc: 0.9821\n",
            "Epoch 293/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0406 - acc: 0.9821\n",
            "Epoch 294/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0404 - acc: 0.9821\n",
            "Epoch 295/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0403 - acc: 0.9821\n",
            "Epoch 296/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0402 - acc: 0.9821\n",
            "Epoch 297/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0400 - acc: 0.9821\n",
            "Epoch 298/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0399 - acc: 0.9821\n",
            "Epoch 299/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0398 - acc: 0.9821\n",
            "Epoch 300/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0396 - acc: 0.9821\n",
            "Epoch 301/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0395 - acc: 0.9821\n",
            "Epoch 302/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0394 - acc: 0.9821\n",
            "Epoch 303/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0392 - acc: 0.9821\n",
            "Epoch 304/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0391 - acc: 0.9821\n",
            "Epoch 305/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0390 - acc: 0.9821\n",
            "Epoch 306/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0388 - acc: 0.9821\n",
            "Epoch 307/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0387 - acc: 0.9821\n",
            "Epoch 308/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0386 - acc: 0.9821\n",
            "Epoch 309/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0384 - acc: 0.9821\n",
            "Epoch 310/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0383 - acc: 0.9821\n",
            "Epoch 311/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0382 - acc: 0.9821\n",
            "Epoch 312/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0380 - acc: 0.9821\n",
            "Epoch 313/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0379 - acc: 0.9821\n",
            "Epoch 314/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0377 - acc: 0.9821\n",
            "Epoch 315/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0376 - acc: 0.9821\n",
            "Epoch 316/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0375 - acc: 0.9821\n",
            "Epoch 317/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0373 - acc: 0.9821\n",
            "Epoch 318/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0372 - acc: 0.9821\n",
            "Epoch 319/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0371 - acc: 0.9821\n",
            "Epoch 320/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0369 - acc: 0.9821\n",
            "Epoch 321/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0368 - acc: 0.9821\n",
            "Epoch 322/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0367 - acc: 0.9821\n",
            "Epoch 323/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0365 - acc: 0.9821\n",
            "Epoch 324/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0364 - acc: 0.9821\n",
            "Epoch 325/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0363 - acc: 0.9821\n",
            "Epoch 326/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0361 - acc: 0.9821\n",
            "Epoch 327/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0360 - acc: 0.9821\n",
            "Epoch 328/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0358 - acc: 0.9821\n",
            "Epoch 329/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0357 - acc: 0.9821\n",
            "Epoch 330/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0356 - acc: 0.9821\n",
            "Epoch 331/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0354 - acc: 0.9821\n",
            "Epoch 332/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0353 - acc: 0.9821\n",
            "Epoch 333/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0352 - acc: 0.9821\n",
            "Epoch 334/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0350 - acc: 0.9821\n",
            "Epoch 335/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0349 - acc: 0.9821\n",
            "Epoch 336/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0348 - acc: 0.9821\n",
            "Epoch 337/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0346 - acc: 0.9821\n",
            "Epoch 338/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0345 - acc: 0.9821\n",
            "Epoch 339/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0344 - acc: 0.9821\n",
            "Epoch 340/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0342 - acc: 0.9821\n",
            "Epoch 341/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0341 - acc: 0.9881\n",
            "Epoch 342/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0340 - acc: 0.9881\n",
            "Epoch 343/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0339 - acc: 0.9881\n",
            "Epoch 344/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0338 - acc: 0.9881\n",
            "Epoch 345/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0337 - acc: 0.9821\n",
            "Epoch 346/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0335 - acc: 0.9821\n",
            "Epoch 347/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0334 - acc: 0.9821\n",
            "Epoch 348/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0333 - acc: 0.9881\n",
            "Epoch 349/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0332 - acc: 0.9881\n",
            "Epoch 350/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0331 - acc: 0.9881\n",
            "Epoch 351/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0330 - acc: 0.9881\n",
            "Epoch 352/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0329 - acc: 0.9881\n",
            "Epoch 353/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0328 - acc: 0.9881\n",
            "Epoch 354/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0327 - acc: 0.9881\n",
            "Epoch 355/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0326 - acc: 0.9881\n",
            "Epoch 356/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0325 - acc: 0.9881\n",
            "Epoch 357/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0324 - acc: 0.9881\n",
            "Epoch 358/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0323 - acc: 0.9881\n",
            "Epoch 359/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0322 - acc: 0.9881\n",
            "Epoch 360/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0321 - acc: 0.9881\n",
            "Epoch 361/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0320 - acc: 0.9881\n",
            "Epoch 362/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0319 - acc: 0.9881\n",
            "Epoch 363/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0318 - acc: 0.9881\n",
            "Epoch 364/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0317 - acc: 0.9881\n",
            "Epoch 365/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0317 - acc: 0.9881\n",
            "Epoch 366/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0316 - acc: 0.9881\n",
            "Epoch 367/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0315 - acc: 0.9881\n",
            "Epoch 368/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0314 - acc: 0.9881\n",
            "Epoch 369/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0313 - acc: 0.9881\n",
            "Epoch 370/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0313 - acc: 0.9881\n",
            "Epoch 371/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0312 - acc: 0.9881\n",
            "Epoch 372/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0311 - acc: 0.9881\n",
            "Epoch 373/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0310 - acc: 0.9881\n",
            "Epoch 374/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0310 - acc: 0.9881\n",
            "Epoch 375/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0309 - acc: 0.9881\n",
            "Epoch 376/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0308 - acc: 0.9881\n",
            "Epoch 377/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0307 - acc: 0.9881\n",
            "Epoch 378/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0307 - acc: 0.9881\n",
            "Epoch 379/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0306 - acc: 0.9881\n",
            "Epoch 380/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0305 - acc: 0.9881\n",
            "Epoch 381/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0305 - acc: 0.9881\n",
            "Epoch 382/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0304 - acc: 0.9881\n",
            "Epoch 383/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0303 - acc: 0.9881\n",
            "Epoch 384/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0302 - acc: 0.9881\n",
            "Epoch 385/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0302 - acc: 0.9881\n",
            "Epoch 386/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0301 - acc: 0.9881\n",
            "Epoch 387/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0300 - acc: 0.9881\n",
            "Epoch 388/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0300 - acc: 0.9881\n",
            "Epoch 389/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0299 - acc: 0.9881\n",
            "Epoch 390/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0298 - acc: 0.9881\n",
            "Epoch 391/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0298 - acc: 0.9881\n",
            "Epoch 392/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0297 - acc: 0.9881\n",
            "Epoch 393/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0297 - acc: 0.9881\n",
            "Epoch 394/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0296 - acc: 0.9881\n",
            "Epoch 395/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0295 - acc: 0.9881\n",
            "Epoch 396/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0295 - acc: 0.9881\n",
            "Epoch 397/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0294 - acc: 0.9881\n",
            "Epoch 398/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0294 - acc: 0.9881\n",
            "Epoch 399/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0293 - acc: 0.9881\n",
            "Epoch 400/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0292 - acc: 0.9881\n",
            "Epoch 401/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0292 - acc: 0.9881\n",
            "Epoch 402/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0291 - acc: 0.9881\n",
            "Epoch 403/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0291 - acc: 0.9881\n",
            "Epoch 404/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0290 - acc: 0.9881\n",
            "Epoch 405/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0290 - acc: 0.9881\n",
            "Epoch 406/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0289 - acc: 0.9881\n",
            "Epoch 407/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0289 - acc: 0.9881\n",
            "Epoch 408/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0288 - acc: 0.9881\n",
            "Epoch 409/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0288 - acc: 0.9881\n",
            "Epoch 410/1000\n",
            "168/168 [==============================] - 0s 27us/step - loss: 0.0287 - acc: 0.9881\n",
            "Epoch 411/1000\n",
            "168/168 [==============================] - 0s 31us/step - loss: 0.0287 - acc: 0.9881\n",
            "Epoch 412/1000\n",
            "168/168 [==============================] - 0s 31us/step - loss: 0.0286 - acc: 0.9881\n",
            "Epoch 413/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0286 - acc: 0.9881\n",
            "Epoch 414/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0285 - acc: 0.9881\n",
            "Epoch 415/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0285 - acc: 0.9881\n",
            "Epoch 416/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0284 - acc: 0.9881\n",
            "Epoch 417/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0284 - acc: 0.9881\n",
            "Epoch 418/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0283 - acc: 0.9881\n",
            "Epoch 419/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0283 - acc: 0.9881\n",
            "Epoch 420/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0282 - acc: 0.9881\n",
            "Epoch 421/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0282 - acc: 0.9881\n",
            "Epoch 422/1000\n",
            "168/168 [==============================] - 0s 24us/step - loss: 0.0281 - acc: 0.9881\n",
            "Epoch 423/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0281 - acc: 0.9881\n",
            "Epoch 424/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0280 - acc: 0.9881\n",
            "Epoch 425/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0280 - acc: 0.9881\n",
            "Epoch 426/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0279 - acc: 0.9881\n",
            "Epoch 427/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0279 - acc: 0.9881\n",
            "Epoch 428/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0278 - acc: 0.9881\n",
            "Epoch 429/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0278 - acc: 0.9881\n",
            "Epoch 430/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0277 - acc: 0.9881\n",
            "Epoch 431/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0277 - acc: 0.9881\n",
            "Epoch 432/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0276 - acc: 0.9881\n",
            "Epoch 433/1000\n",
            "168/168 [==============================] - 0s 22us/step - loss: 0.0276 - acc: 0.9881\n",
            "Epoch 434/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0275 - acc: 0.9881\n",
            "Epoch 435/1000\n",
            "168/168 [==============================] - 0s 22us/step - loss: 0.0275 - acc: 0.9881\n",
            "Epoch 436/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0274 - acc: 0.9881\n",
            "Epoch 437/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0274 - acc: 0.9881\n",
            "Epoch 438/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0273 - acc: 0.9881\n",
            "Epoch 439/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0273 - acc: 0.9881\n",
            "Epoch 440/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0272 - acc: 0.9881\n",
            "Epoch 441/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0272 - acc: 0.9881\n",
            "Epoch 442/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0271 - acc: 0.9881\n",
            "Epoch 443/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0271 - acc: 0.9881\n",
            "Epoch 444/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0270 - acc: 0.9881\n",
            "Epoch 445/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0270 - acc: 0.9881\n",
            "Epoch 446/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0269 - acc: 0.9881\n",
            "Epoch 447/1000\n",
            "168/168 [==============================] - 0s 22us/step - loss: 0.0269 - acc: 0.9881\n",
            "Epoch 448/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0268 - acc: 0.9881\n",
            "Epoch 449/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0268 - acc: 0.9881\n",
            "Epoch 450/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0267 - acc: 0.9881\n",
            "Epoch 451/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0267 - acc: 0.9881\n",
            "Epoch 452/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0266 - acc: 0.9881\n",
            "Epoch 453/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0266 - acc: 0.9881\n",
            "Epoch 454/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0265 - acc: 0.9881\n",
            "Epoch 455/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0265 - acc: 0.9881\n",
            "Epoch 456/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0264 - acc: 0.9881\n",
            "Epoch 457/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0264 - acc: 0.9881\n",
            "Epoch 458/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0263 - acc: 0.9881\n",
            "Epoch 459/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0263 - acc: 0.9881\n",
            "Epoch 460/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0262 - acc: 0.9881\n",
            "Epoch 461/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0262 - acc: 0.9881\n",
            "Epoch 462/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0262 - acc: 0.9881\n",
            "Epoch 463/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0261 - acc: 0.9881\n",
            "Epoch 464/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0261 - acc: 0.9881\n",
            "Epoch 465/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0260 - acc: 0.9881\n",
            "Epoch 466/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0260 - acc: 0.9881\n",
            "Epoch 467/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0259 - acc: 0.9881\n",
            "Epoch 468/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0259 - acc: 0.9881\n",
            "Epoch 469/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0259 - acc: 0.9881\n",
            "Epoch 470/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0258 - acc: 0.9881\n",
            "Epoch 471/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0258 - acc: 0.9881\n",
            "Epoch 472/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0257 - acc: 0.9881\n",
            "Epoch 473/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0257 - acc: 0.9881\n",
            "Epoch 474/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0257 - acc: 0.9881\n",
            "Epoch 475/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0256 - acc: 0.9881\n",
            "Epoch 476/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0256 - acc: 0.9881\n",
            "Epoch 477/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0255 - acc: 0.9881\n",
            "Epoch 478/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0255 - acc: 0.9881\n",
            "Epoch 479/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0255 - acc: 0.9881\n",
            "Epoch 480/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0254 - acc: 0.9881\n",
            "Epoch 481/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0254 - acc: 0.9881\n",
            "Epoch 482/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0254 - acc: 0.9881\n",
            "Epoch 483/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0253 - acc: 0.9881\n",
            "Epoch 484/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0253 - acc: 0.9881\n",
            "Epoch 485/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0252 - acc: 0.9881\n",
            "Epoch 486/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0252 - acc: 0.9881\n",
            "Epoch 487/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0252 - acc: 0.9881\n",
            "Epoch 488/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0251 - acc: 0.9881\n",
            "Epoch 489/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0251 - acc: 0.9881\n",
            "Epoch 490/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0251 - acc: 0.9881\n",
            "Epoch 491/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0250 - acc: 0.9881\n",
            "Epoch 492/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0250 - acc: 0.9881\n",
            "Epoch 493/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0250 - acc: 0.9881\n",
            "Epoch 494/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0249 - acc: 0.9881\n",
            "Epoch 495/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0249 - acc: 0.9881\n",
            "Epoch 496/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0248 - acc: 0.9881\n",
            "Epoch 497/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0248 - acc: 0.9881\n",
            "Epoch 498/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0248 - acc: 0.9881\n",
            "Epoch 499/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0248 - acc: 0.9881\n",
            "Epoch 500/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0247 - acc: 0.9881\n",
            "Epoch 501/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0247 - acc: 0.9881\n",
            "Epoch 502/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0247 - acc: 0.9881\n",
            "Epoch 503/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0246 - acc: 0.9881\n",
            "Epoch 504/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0246 - acc: 0.9881\n",
            "Epoch 505/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0246 - acc: 0.9881\n",
            "Epoch 506/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0246 - acc: 0.9881\n",
            "Epoch 507/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0245 - acc: 0.9881\n",
            "Epoch 508/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0245 - acc: 0.9881\n",
            "Epoch 509/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0245 - acc: 0.9881\n",
            "Epoch 510/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0245 - acc: 0.9881\n",
            "Epoch 511/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0244 - acc: 0.9881\n",
            "Epoch 512/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0244 - acc: 0.9881\n",
            "Epoch 513/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0244 - acc: 0.9881\n",
            "Epoch 514/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0244 - acc: 0.9881\n",
            "Epoch 515/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0244 - acc: 0.9881\n",
            "Epoch 516/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0243 - acc: 0.9881\n",
            "Epoch 517/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0243 - acc: 0.9881\n",
            "Epoch 518/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0243 - acc: 0.9881\n",
            "Epoch 519/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0243 - acc: 0.9881\n",
            "Epoch 520/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0242 - acc: 0.9881\n",
            "Epoch 521/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0242 - acc: 0.9881\n",
            "Epoch 522/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0242 - acc: 0.9881\n",
            "Epoch 523/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0242 - acc: 0.9881\n",
            "Epoch 524/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0241 - acc: 0.9881\n",
            "Epoch 525/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0241 - acc: 0.9881\n",
            "Epoch 526/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0241 - acc: 0.9881\n",
            "Epoch 527/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0241 - acc: 0.9881\n",
            "Epoch 528/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0240 - acc: 0.9881\n",
            "Epoch 529/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0240 - acc: 0.9881\n",
            "Epoch 530/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0240 - acc: 0.9881\n",
            "Epoch 531/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0240 - acc: 0.9881\n",
            "Epoch 532/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0240 - acc: 0.9881\n",
            "Epoch 533/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0239 - acc: 0.9881\n",
            "Epoch 534/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0239 - acc: 0.9881\n",
            "Epoch 535/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0239 - acc: 0.9881\n",
            "Epoch 536/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0239 - acc: 0.9881\n",
            "Epoch 537/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0238 - acc: 0.9881\n",
            "Epoch 538/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0238 - acc: 0.9881\n",
            "Epoch 539/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0238 - acc: 0.9881\n",
            "Epoch 540/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0238 - acc: 0.9881\n",
            "Epoch 541/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0238 - acc: 0.9881\n",
            "Epoch 542/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9881\n",
            "Epoch 543/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0237 - acc: 0.9881\n",
            "Epoch 544/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0237 - acc: 0.9881\n",
            "Epoch 545/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0237 - acc: 0.9881\n",
            "Epoch 546/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0236 - acc: 0.9881\n",
            "Epoch 547/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0236 - acc: 0.9881\n",
            "Epoch 548/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0236 - acc: 0.9881\n",
            "Epoch 549/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0236 - acc: 0.9881\n",
            "Epoch 550/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0236 - acc: 0.9881\n",
            "Epoch 551/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0235 - acc: 0.9881\n",
            "Epoch 552/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0235 - acc: 0.9881\n",
            "Epoch 553/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0235 - acc: 0.9881\n",
            "Epoch 554/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0235 - acc: 0.9881\n",
            "Epoch 555/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0234 - acc: 0.9881\n",
            "Epoch 556/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0234 - acc: 0.9881\n",
            "Epoch 557/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0234 - acc: 0.9881\n",
            "Epoch 558/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0234 - acc: 0.9881\n",
            "Epoch 559/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0234 - acc: 0.9881\n",
            "Epoch 560/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0233 - acc: 0.9881\n",
            "Epoch 561/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9881\n",
            "Epoch 562/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0233 - acc: 0.9881\n",
            "Epoch 563/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0233 - acc: 0.9881\n",
            "Epoch 564/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0233 - acc: 0.9881\n",
            "Epoch 565/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0232 - acc: 0.9881\n",
            "Epoch 566/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9881\n",
            "Epoch 567/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0232 - acc: 0.9881\n",
            "Epoch 568/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0232 - acc: 0.9881\n",
            "Epoch 569/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9881\n",
            "Epoch 570/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9881\n",
            "Epoch 571/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9881\n",
            "Epoch 572/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9881\n",
            "Epoch 573/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9881\n",
            "Epoch 574/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9881\n",
            "Epoch 575/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0230 - acc: 0.9881\n",
            "Epoch 576/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0230 - acc: 0.9881\n",
            "Epoch 577/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0230 - acc: 0.9881\n",
            "Epoch 578/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0230 - acc: 0.9881\n",
            "Epoch 579/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0230 - acc: 0.9881\n",
            "Epoch 580/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0229 - acc: 0.9881\n",
            "Epoch 581/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0229 - acc: 0.9881\n",
            "Epoch 582/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0229 - acc: 0.9881\n",
            "Epoch 583/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0229 - acc: 0.9881\n",
            "Epoch 584/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0229 - acc: 0.9881\n",
            "Epoch 585/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0228 - acc: 0.9881\n",
            "Epoch 586/1000\n",
            "168/168 [==============================] - 0s 23us/step - loss: 0.0228 - acc: 0.9881\n",
            "Epoch 587/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0228 - acc: 0.9881\n",
            "Epoch 588/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0228 - acc: 0.9881\n",
            "Epoch 589/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0228 - acc: 0.9881\n",
            "Epoch 590/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0227 - acc: 0.9881\n",
            "Epoch 591/1000\n",
            "168/168 [==============================] - 0s 23us/step - loss: 0.0227 - acc: 0.9881\n",
            "Epoch 592/1000\n",
            "168/168 [==============================] - 0s 24us/step - loss: 0.0227 - acc: 0.9881\n",
            "Epoch 593/1000\n",
            "168/168 [==============================] - 0s 27us/step - loss: 0.0227 - acc: 0.9881\n",
            "Epoch 594/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0227 - acc: 0.9881\n",
            "Epoch 595/1000\n",
            "168/168 [==============================] - 0s 24us/step - loss: 0.0226 - acc: 0.9881\n",
            "Epoch 596/1000\n",
            "168/168 [==============================] - 0s 10us/step - loss: 0.0226 - acc: 0.9881\n",
            "Epoch 597/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0226 - acc: 0.9881\n",
            "Epoch 598/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0226 - acc: 0.9881\n",
            "Epoch 599/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0226 - acc: 0.9881\n",
            "Epoch 600/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0226 - acc: 0.9881\n",
            "Epoch 601/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9881\n",
            "Epoch 602/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0225 - acc: 0.9881\n",
            "Epoch 603/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0225 - acc: 0.9881\n",
            "Epoch 604/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0225 - acc: 0.9881\n",
            "Epoch 605/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0224 - acc: 0.9881\n",
            "Epoch 606/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9881\n",
            "Epoch 607/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0224 - acc: 0.9881\n",
            "Epoch 608/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0224 - acc: 0.9881\n",
            "Epoch 609/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0224 - acc: 0.9881\n",
            "Epoch 610/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0223 - acc: 0.9881\n",
            "Epoch 611/1000\n",
            "168/168 [==============================] - 0s 69us/step - loss: 0.0223 - acc: 0.9881\n",
            "Epoch 612/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0223 - acc: 0.9881\n",
            "Epoch 613/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9881\n",
            "Epoch 614/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9881\n",
            "Epoch 615/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0222 - acc: 0.9881\n",
            "Epoch 616/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0222 - acc: 0.9881\n",
            "Epoch 617/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0222 - acc: 0.9881\n",
            "Epoch 618/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0222 - acc: 0.9881\n",
            "Epoch 619/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0222 - acc: 0.9881\n",
            "Epoch 620/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0221 - acc: 0.9881\n",
            "Epoch 621/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0221 - acc: 0.9881\n",
            "Epoch 622/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0221 - acc: 0.9881\n",
            "Epoch 623/1000\n",
            "168/168 [==============================] - 0s 9us/step - loss: 0.0221 - acc: 0.9881\n",
            "Epoch 624/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0221 - acc: 0.9881\n",
            "Epoch 625/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0220 - acc: 0.9881\n",
            "Epoch 626/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0220 - acc: 0.9881\n",
            "Epoch 627/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0220 - acc: 0.9881\n",
            "Epoch 628/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0220 - acc: 0.9881\n",
            "Epoch 629/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0220 - acc: 0.9881\n",
            "Epoch 630/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9881\n",
            "Epoch 631/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0219 - acc: 0.9881\n",
            "Epoch 632/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9881\n",
            "Epoch 633/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0219 - acc: 0.9881\n",
            "Epoch 634/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0219 - acc: 0.9881\n",
            "Epoch 635/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0218 - acc: 0.9881\n",
            "Epoch 636/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0218 - acc: 0.9881\n",
            "Epoch 637/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9881\n",
            "Epoch 638/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9881\n",
            "Epoch 639/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0218 - acc: 0.9881\n",
            "Epoch 640/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0217 - acc: 0.9881\n",
            "Epoch 641/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9881\n",
            "Epoch 642/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0217 - acc: 0.9881\n",
            "Epoch 643/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9881\n",
            "Epoch 644/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0217 - acc: 0.9881\n",
            "Epoch 645/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0216 - acc: 0.9881\n",
            "Epoch 646/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0216 - acc: 0.9881\n",
            "Epoch 647/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0216 - acc: 0.9881\n",
            "Epoch 648/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0216 - acc: 0.9881\n",
            "Epoch 649/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0216 - acc: 0.9881\n",
            "Epoch 650/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0216 - acc: 0.9881\n",
            "Epoch 651/1000\n",
            "168/168 [==============================] - 0s 10us/step - loss: 0.0215 - acc: 0.9881\n",
            "Epoch 652/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0215 - acc: 0.9881\n",
            "Epoch 653/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0215 - acc: 0.9881\n",
            "Epoch 654/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0215 - acc: 0.9881\n",
            "Epoch 655/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0215 - acc: 0.9881\n",
            "Epoch 656/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0214 - acc: 0.9881\n",
            "Epoch 657/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0214 - acc: 0.9881\n",
            "Epoch 658/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0214 - acc: 0.9881\n",
            "Epoch 659/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0214 - acc: 0.9881\n",
            "Epoch 660/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0214 - acc: 0.9881\n",
            "Epoch 661/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0214 - acc: 0.9881\n",
            "Epoch 662/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0213 - acc: 0.9881\n",
            "Epoch 663/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9881\n",
            "Epoch 664/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0213 - acc: 0.9881\n",
            "Epoch 665/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0213 - acc: 0.9881\n",
            "Epoch 666/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9881\n",
            "Epoch 667/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0212 - acc: 0.9881\n",
            "Epoch 668/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0212 - acc: 0.9881\n",
            "Epoch 669/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0212 - acc: 0.9881\n",
            "Epoch 670/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0212 - acc: 0.9881\n",
            "Epoch 671/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0212 - acc: 0.9881\n",
            "Epoch 672/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0212 - acc: 0.9881\n",
            "Epoch 673/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0211 - acc: 0.9881\n",
            "Epoch 674/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0211 - acc: 0.9881\n",
            "Epoch 675/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0211 - acc: 0.9881\n",
            "Epoch 676/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0211 - acc: 0.9881\n",
            "Epoch 677/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0211 - acc: 0.9881\n",
            "Epoch 678/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0210 - acc: 0.9881\n",
            "Epoch 679/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0210 - acc: 0.9881\n",
            "Epoch 680/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9881\n",
            "Epoch 681/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9881\n",
            "Epoch 682/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0210 - acc: 0.9881\n",
            "Epoch 683/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0210 - acc: 0.9881\n",
            "Epoch 684/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9881\n",
            "Epoch 685/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0209 - acc: 0.9881\n",
            "Epoch 686/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0209 - acc: 0.9881\n",
            "Epoch 687/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0209 - acc: 0.9881\n",
            "Epoch 688/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0209 - acc: 0.9881\n",
            "Epoch 689/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0209 - acc: 0.9881\n",
            "Epoch 690/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0208 - acc: 0.9881\n",
            "Epoch 691/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0208 - acc: 0.9881\n",
            "Epoch 692/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0208 - acc: 0.9881\n",
            "Epoch 693/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0208 - acc: 0.9881\n",
            "Epoch 694/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0208 - acc: 0.9881\n",
            "Epoch 695/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0207 - acc: 0.9881\n",
            "Epoch 696/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0207 - acc: 0.9881\n",
            "Epoch 697/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0207 - acc: 0.9881\n",
            "Epoch 698/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0207 - acc: 0.9881\n",
            "Epoch 699/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0207 - acc: 0.9881\n",
            "Epoch 700/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0207 - acc: 0.9881\n",
            "Epoch 701/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0206 - acc: 0.9881\n",
            "Epoch 702/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0206 - acc: 0.9881\n",
            "Epoch 703/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0206 - acc: 0.9881\n",
            "Epoch 704/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0206 - acc: 0.9881\n",
            "Epoch 705/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0206 - acc: 0.9881\n",
            "Epoch 706/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0205 - acc: 0.9881\n",
            "Epoch 707/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9881\n",
            "Epoch 708/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0205 - acc: 0.9881\n",
            "Epoch 709/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0205 - acc: 0.9881\n",
            "Epoch 710/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0205 - acc: 0.9881\n",
            "Epoch 711/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0205 - acc: 0.9881\n",
            "Epoch 712/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0204 - acc: 0.9881\n",
            "Epoch 713/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9881\n",
            "Epoch 714/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0204 - acc: 0.9881\n",
            "Epoch 715/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0204 - acc: 0.9881\n",
            "Epoch 716/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0204 - acc: 0.9881\n",
            "Epoch 717/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0203 - acc: 0.9881\n",
            "Epoch 718/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0203 - acc: 0.9881\n",
            "Epoch 719/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0203 - acc: 0.9881\n",
            "Epoch 720/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0203 - acc: 0.9881\n",
            "Epoch 721/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0203 - acc: 0.9881\n",
            "Epoch 722/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0203 - acc: 0.9881\n",
            "Epoch 723/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0202 - acc: 0.9881\n",
            "Epoch 724/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0202 - acc: 0.9881\n",
            "Epoch 725/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0202 - acc: 0.9881\n",
            "Epoch 726/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0202 - acc: 0.9881\n",
            "Epoch 727/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0202 - acc: 0.9881\n",
            "Epoch 728/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9881\n",
            "Epoch 729/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9881\n",
            "Epoch 730/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9881\n",
            "Epoch 731/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0201 - acc: 0.9881\n",
            "Epoch 732/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0201 - acc: 0.9881\n",
            "Epoch 733/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9881\n",
            "Epoch 734/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9881\n",
            "Epoch 735/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0200 - acc: 0.9881\n",
            "Epoch 736/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9881\n",
            "Epoch 737/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0200 - acc: 0.9881\n",
            "Epoch 738/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0200 - acc: 0.9881\n",
            "Epoch 739/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0199 - acc: 0.9881\n",
            "Epoch 740/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0199 - acc: 0.9881\n",
            "Epoch 741/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0199 - acc: 0.9881\n",
            "Epoch 742/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0199 - acc: 0.9881\n",
            "Epoch 743/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0199 - acc: 0.9881\n",
            "Epoch 744/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0198 - acc: 0.9881\n",
            "Epoch 745/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9881\n",
            "Epoch 746/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0198 - acc: 0.9881\n",
            "Epoch 747/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0198 - acc: 0.9881\n",
            "Epoch 748/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0198 - acc: 0.9881\n",
            "Epoch 749/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0197 - acc: 0.9881\n",
            "Epoch 750/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0197 - acc: 0.9881\n",
            "Epoch 751/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0197 - acc: 0.9881\n",
            "Epoch 752/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0197 - acc: 0.9881\n",
            "Epoch 753/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0197 - acc: 0.9881\n",
            "Epoch 754/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0196 - acc: 0.9881\n",
            "Epoch 755/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0196 - acc: 0.9881\n",
            "Epoch 756/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0196 - acc: 0.9881\n",
            "Epoch 757/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0196 - acc: 0.9881\n",
            "Epoch 758/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9881\n",
            "Epoch 759/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9881\n",
            "Epoch 760/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9881\n",
            "Epoch 761/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9881\n",
            "Epoch 762/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0195 - acc: 0.9881\n",
            "Epoch 763/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0195 - acc: 0.9881\n",
            "Epoch 764/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0194 - acc: 0.9881\n",
            "Epoch 765/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0194 - acc: 0.9881\n",
            "Epoch 766/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0194 - acc: 0.9881\n",
            "Epoch 767/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9881\n",
            "Epoch 768/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0194 - acc: 0.9881\n",
            "Epoch 769/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9881\n",
            "Epoch 770/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9881\n",
            "Epoch 771/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9881\n",
            "Epoch 772/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0193 - acc: 0.9881\n",
            "Epoch 773/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9881\n",
            "Epoch 774/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9881\n",
            "Epoch 775/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9881\n",
            "Epoch 776/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0192 - acc: 0.9881\n",
            "Epoch 777/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0192 - acc: 0.9881\n",
            "Epoch 778/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9881\n",
            "Epoch 779/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0192 - acc: 0.9881\n",
            "Epoch 780/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0192 - acc: 0.9881\n",
            "Epoch 781/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9881\n",
            "Epoch 782/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9881\n",
            "Epoch 783/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9881\n",
            "Epoch 784/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9881\n",
            "Epoch 785/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9881\n",
            "Epoch 786/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0190 - acc: 0.9881\n",
            "Epoch 787/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0190 - acc: 0.9881\n",
            "Epoch 788/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0190 - acc: 0.9881\n",
            "Epoch 789/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0190 - acc: 0.9881\n",
            "Epoch 790/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0190 - acc: 0.9881\n",
            "Epoch 791/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0189 - acc: 0.9881\n",
            "Epoch 792/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9881\n",
            "Epoch 793/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9881\n",
            "Epoch 794/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9881\n",
            "Epoch 795/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9881\n",
            "Epoch 796/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0189 - acc: 0.9881\n",
            "Epoch 797/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0188 - acc: 0.9881\n",
            "Epoch 798/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0188 - acc: 0.9881\n",
            "Epoch 799/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0188 - acc: 0.9881\n",
            "Epoch 800/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9881\n",
            "Epoch 801/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0188 - acc: 0.9881\n",
            "Epoch 802/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9881\n",
            "Epoch 803/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0187 - acc: 0.9881\n",
            "Epoch 804/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0187 - acc: 0.9881\n",
            "Epoch 805/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0187 - acc: 0.9881\n",
            "Epoch 806/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0187 - acc: 0.9881\n",
            "Epoch 807/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0187 - acc: 0.9881\n",
            "Epoch 808/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0186 - acc: 0.9881\n",
            "Epoch 809/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0186 - acc: 0.9881\n",
            "Epoch 810/1000\n",
            "168/168 [==============================] - 0s 41us/step - loss: 0.0186 - acc: 0.9881\n",
            "Epoch 811/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0186 - acc: 0.9881\n",
            "Epoch 812/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0186 - acc: 0.9881\n",
            "Epoch 813/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9881\n",
            "Epoch 814/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9881\n",
            "Epoch 815/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0185 - acc: 0.9881\n",
            "Epoch 816/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9881\n",
            "Epoch 817/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9881\n",
            "Epoch 818/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9881\n",
            "Epoch 819/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9881\n",
            "Epoch 820/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9881\n",
            "Epoch 821/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0184 - acc: 0.9881\n",
            "Epoch 822/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9881\n",
            "Epoch 823/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9881\n",
            "Epoch 824/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9881\n",
            "Epoch 825/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9881\n",
            "Epoch 826/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9881\n",
            "Epoch 827/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9881\n",
            "Epoch 828/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0183 - acc: 0.9881\n",
            "Epoch 829/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9881\n",
            "Epoch 830/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0182 - acc: 0.9881\n",
            "Epoch 831/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0182 - acc: 0.9881\n",
            "Epoch 832/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0182 - acc: 0.9881\n",
            "Epoch 833/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0182 - acc: 0.9881\n",
            "Epoch 834/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0182 - acc: 0.9881\n",
            "Epoch 835/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0182 - acc: 0.9881\n",
            "Epoch 836/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0181 - acc: 0.9881\n",
            "Epoch 837/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0181 - acc: 0.9881\n",
            "Epoch 838/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0181 - acc: 0.9881\n",
            "Epoch 839/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0181 - acc: 0.9881\n",
            "Epoch 840/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0181 - acc: 0.9881\n",
            "Epoch 841/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0180 - acc: 0.9881\n",
            "Epoch 842/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0180 - acc: 0.9881\n",
            "Epoch 843/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0180 - acc: 0.9881\n",
            "Epoch 844/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0180 - acc: 0.9881\n",
            "Epoch 845/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0180 - acc: 0.9881\n",
            "Epoch 846/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0180 - acc: 0.9881\n",
            "Epoch 847/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9881\n",
            "Epoch 848/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0179 - acc: 0.9881\n",
            "Epoch 849/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9881\n",
            "Epoch 850/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9881\n",
            "Epoch 851/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9881\n",
            "Epoch 852/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9881\n",
            "Epoch 853/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9881\n",
            "Epoch 854/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9881\n",
            "Epoch 855/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9881\n",
            "Epoch 856/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0178 - acc: 0.9881\n",
            "Epoch 857/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0178 - acc: 0.9881\n",
            "Epoch 858/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0177 - acc: 0.9881\n",
            "Epoch 859/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9881\n",
            "Epoch 860/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9881\n",
            "Epoch 861/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9881\n",
            "Epoch 862/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9881\n",
            "Epoch 863/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9881\n",
            "Epoch 864/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0176 - acc: 0.9881\n",
            "Epoch 865/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9881\n",
            "Epoch 866/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9881\n",
            "Epoch 867/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0176 - acc: 0.9881\n",
            "Epoch 868/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9881\n",
            "Epoch 869/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9881\n",
            "Epoch 870/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0175 - acc: 0.9881\n",
            "Epoch 871/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0175 - acc: 0.9881\n",
            "Epoch 872/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0175 - acc: 0.9881\n",
            "Epoch 873/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0175 - acc: 0.9881\n",
            "Epoch 874/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0175 - acc: 0.9881\n",
            "Epoch 875/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0175 - acc: 0.9881\n",
            "Epoch 876/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0174 - acc: 0.9881\n",
            "Epoch 877/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9881\n",
            "Epoch 878/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9881\n",
            "Epoch 879/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0174 - acc: 0.9881\n",
            "Epoch 880/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0174 - acc: 0.9881\n",
            "Epoch 881/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9881\n",
            "Epoch 882/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9881\n",
            "Epoch 883/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0173 - acc: 0.9881\n",
            "Epoch 884/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0173 - acc: 0.9881\n",
            "Epoch 885/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0173 - acc: 0.9881\n",
            "Epoch 886/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9881\n",
            "Epoch 887/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0173 - acc: 0.9881\n",
            "Epoch 888/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0172 - acc: 0.9881\n",
            "Epoch 889/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0172 - acc: 0.9881\n",
            "Epoch 890/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0172 - acc: 0.9881\n",
            "Epoch 891/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0172 - acc: 0.9881\n",
            "Epoch 892/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0172 - acc: 0.9881\n",
            "Epoch 893/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9881\n",
            "Epoch 894/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0171 - acc: 0.9881\n",
            "Epoch 895/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0171 - acc: 0.9881\n",
            "Epoch 896/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0171 - acc: 0.9881\n",
            "Epoch 897/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9881\n",
            "Epoch 898/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9881\n",
            "Epoch 899/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0170 - acc: 0.9881\n",
            "Epoch 900/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9881\n",
            "Epoch 901/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9881\n",
            "Epoch 902/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9881\n",
            "Epoch 903/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0170 - acc: 0.9881\n",
            "Epoch 904/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0170 - acc: 0.9881\n",
            "Epoch 905/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0169 - acc: 0.9881\n",
            "Epoch 906/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0169 - acc: 0.9881\n",
            "Epoch 907/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0169 - acc: 0.9881\n",
            "Epoch 908/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9881\n",
            "Epoch 909/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0169 - acc: 0.9881\n",
            "Epoch 910/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9881\n",
            "Epoch 911/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9881\n",
            "Epoch 912/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0168 - acc: 0.9881\n",
            "Epoch 913/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9881\n",
            "Epoch 914/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9881\n",
            "Epoch 915/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0168 - acc: 0.9881\n",
            "Epoch 916/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0167 - acc: 0.9881\n",
            "Epoch 917/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9881\n",
            "Epoch 918/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9881\n",
            "Epoch 919/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0167 - acc: 0.9881\n",
            "Epoch 920/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0167 - acc: 0.9881\n",
            "Epoch 921/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0167 - acc: 0.9881\n",
            "Epoch 922/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0166 - acc: 0.9881\n",
            "Epoch 923/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0166 - acc: 0.9881\n",
            "Epoch 924/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0166 - acc: 0.9881\n",
            "Epoch 925/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0166 - acc: 0.9881\n",
            "Epoch 926/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0166 - acc: 0.9881\n",
            "Epoch 927/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9881\n",
            "Epoch 928/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0165 - acc: 0.9881\n",
            "Epoch 929/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0165 - acc: 0.9881\n",
            "Epoch 930/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0165 - acc: 0.9881\n",
            "Epoch 931/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0165 - acc: 0.9881\n",
            "Epoch 932/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9881\n",
            "Epoch 933/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0164 - acc: 0.9881\n",
            "Epoch 934/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0164 - acc: 0.9881\n",
            "Epoch 935/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0164 - acc: 0.9881\n",
            "Epoch 936/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0164 - acc: 0.9881\n",
            "Epoch 937/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9881\n",
            "Epoch 938/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0164 - acc: 0.9881\n",
            "Epoch 939/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0163 - acc: 0.9881\n",
            "Epoch 940/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9881\n",
            "Epoch 941/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9881\n",
            "Epoch 942/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0163 - acc: 0.9881\n",
            "Epoch 943/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0163 - acc: 0.9881\n",
            "Epoch 944/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9881\n",
            "Epoch 945/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9881\n",
            "Epoch 946/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9881\n",
            "Epoch 947/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0162 - acc: 0.9881\n",
            "Epoch 948/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9881\n",
            "Epoch 949/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0162 - acc: 0.9881\n",
            "Epoch 950/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0161 - acc: 0.9881\n",
            "Epoch 951/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0161 - acc: 0.9881\n",
            "Epoch 952/1000\n",
            "168/168 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9881\n",
            "Epoch 953/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0161 - acc: 0.9881\n",
            "Epoch 954/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0161 - acc: 0.9881\n",
            "Epoch 955/1000\n",
            "168/168 [==============================] - 0s 16us/step - loss: 0.0161 - acc: 0.9881\n",
            "Epoch 956/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0160 - acc: 0.9881\n",
            "Epoch 957/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0160 - acc: 0.9881\n",
            "Epoch 958/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0160 - acc: 0.9881\n",
            "Epoch 959/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0160 - acc: 0.9881\n",
            "Epoch 960/1000\n",
            "168/168 [==============================] - 0s 27us/step - loss: 0.0160 - acc: 0.9881\n",
            "Epoch 961/1000\n",
            "168/168 [==============================] - 0s 26us/step - loss: 0.0160 - acc: 0.9881\n",
            "Epoch 962/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0159 - acc: 0.9881\n",
            "Epoch 963/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0159 - acc: 0.9881\n",
            "Epoch 964/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0159 - acc: 0.9881\n",
            "Epoch 965/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0159 - acc: 0.9881\n",
            "Epoch 966/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0159 - acc: 0.9881\n",
            "Epoch 967/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0159 - acc: 0.9881\n",
            "Epoch 968/1000\n",
            "168/168 [==============================] - 0s 22us/step - loss: 0.0158 - acc: 0.9881\n",
            "Epoch 969/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0158 - acc: 0.9881\n",
            "Epoch 970/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0158 - acc: 0.9881\n",
            "Epoch 971/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0158 - acc: 0.9881\n",
            "Epoch 972/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0158 - acc: 0.9881\n",
            "Epoch 973/1000\n",
            "168/168 [==============================] - 0s 21us/step - loss: 0.0158 - acc: 0.9881\n",
            "Epoch 974/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9881\n",
            "Epoch 975/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0157 - acc: 0.9881\n",
            "Epoch 976/1000\n",
            "168/168 [==============================] - 0s 19us/step - loss: 0.0157 - acc: 0.9881\n",
            "Epoch 977/1000\n",
            "168/168 [==============================] - 0s 18us/step - loss: 0.0157 - acc: 0.9881\n",
            "Epoch 978/1000\n",
            "168/168 [==============================] - 0s 23us/step - loss: 0.0157 - acc: 0.9881\n",
            "Epoch 979/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0157 - acc: 0.9881\n",
            "Epoch 980/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0156 - acc: 0.9881\n",
            "Epoch 981/1000\n",
            "168/168 [==============================] - 0s 20us/step - loss: 0.0156 - acc: 0.9881\n",
            "Epoch 982/1000\n",
            "168/168 [==============================] - 0s 24us/step - loss: 0.0156 - acc: 0.9881\n",
            "Epoch 983/1000\n",
            "168/168 [==============================] - 0s 41us/step - loss: 0.0156 - acc: 0.9881\n",
            "Epoch 984/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0156 - acc: 0.9881\n",
            "Epoch 985/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9881\n",
            "Epoch 986/1000\n",
            "168/168 [==============================] - 0s 17us/step - loss: 0.0155 - acc: 0.9881\n",
            "Epoch 987/1000\n",
            "168/168 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9881\n",
            "Epoch 988/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0155 - acc: 0.9881\n",
            "Epoch 989/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9881\n",
            "Epoch 990/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0155 - acc: 0.9881\n",
            "Epoch 991/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0154 - acc: 0.9881\n",
            "Epoch 992/1000\n",
            "168/168 [==============================] - 0s 10us/step - loss: 0.0154 - acc: 0.9881\n",
            "Epoch 993/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0154 - acc: 0.9881\n",
            "Epoch 994/1000\n",
            "168/168 [==============================] - 0s 11us/step - loss: 0.0154 - acc: 0.9881\n",
            "Epoch 995/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0154 - acc: 0.9881\n",
            "Epoch 996/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0154 - acc: 0.9881\n",
            "Epoch 997/1000\n",
            "168/168 [==============================] - 0s 12us/step - loss: 0.0153 - acc: 0.9881\n",
            "Epoch 998/1000\n",
            "168/168 [==============================] - 0s 10us/step - loss: 0.0153 - acc: 0.9881\n",
            "Epoch 999/1000\n",
            "168/168 [==============================] - 0s 9us/step - loss: 0.0153 - acc: 0.9881\n",
            "Epoch 1000/1000\n",
            "168/168 [==============================] - 0s 13us/step - loss: 0.0153 - acc: 0.9881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMRkutpxtAlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "8a5bd28c-aadd-4e15-b86b-9bead1b85c58"
      },
      "source": [
        "#Visualize the models loss and accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model accuracy and loss')\n",
        "plt.ylabel('accuracy and loss')\n",
        "plt.xlabel('epoch')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd5xddZ3/8dd7SnpPJr0CoSQECESK\n7YeALiLNXQtYVtGVn64K2MF1lUV/qy67YkOERSyolLVtlkWRjqCU0CJJiMSQkISUSe9lZj6/P86Z\ncGcyk7mT3DPnztz38/E4j3vK997zOXOS+7nn+z3n+1VEYGZmlasq7wDMzCxfTgRmZhXOicDMrMI5\nEZiZVTgnAjOzCudEYGZW4ZwILHeSfiTpK0WWXSLpjKxjqhSSJksKSTXtbPffuwI4EZiZVTgnArMS\nae9XtVm5cyKwoqRVBJ+RNFfSNkk/kDRK0m8lbZF0j6ShBeXPlTRP0kZJD0g6qmDbTElPpe+7DejT\nal9nS3omfe8fJR1TZIxvkfS0pM2Slkm6stX216aftzHd/v50fV9J/yFpqaRNkh5O150qaXkbf4cz\n0vkrJf1C0k8lbQbeL+lESX9K97FS0ncl9Sp4/3RJd0taL2m1pM9LGi1pu6ThBeWOl1QvqbaN4+xo\nHyHpw5JeSMtcK0nptmpJ/y5praTFwFuK+dum7+0t6ZuSXk6nb0rqnW4bIemOdH/rJf1BUlW67XOS\nVqTne6Gk04vdp3WRiPDkqcMJWAI8CowCxgFrgKeAmSRf5PcBX0rLHg5sA94I1AKfBRYBvdJpKfCJ\ndNvbgD3AV9L3zkw/+ySgGnhfuu/eBXGc0U6MpwIzSH7gHAOsBs5Pt00CtgAXpvsdDhyXbrsWeCA9\nrmrg1UDv9POWt/F3OCOdvzKN/fx0n32BE4CTgRpgMrAAuCwtPxBYCXwq/ZsNBE5Kt90JfKRgP9cA\n32nnONvdR7o9gDuAIcBEoB44M932YeB5YAIwDLg/LV+zn/PefLxXpf8GRgJ1wB+BL6fbvgp8P/3b\n1gKvAwQcASwDxqblJgOH5v3v2VOr85x3AJ66x5R+Iby7YPmXwHUFyx8HfpPO/zNwe8G2KmBF+sX6\neuBlQAXb/8grieC65i+Xgu0Lgf9TEEebiaCNmL8JXJPOXwH8uo0yVcAO4Ng2thWTCB7qIIbLmvdL\nkoSebqfcO4FH0vlqYBVwYpHHeVnhsaVf7K8tWL4duDydvw/4cMG2N3UiEfwVOKtg298AS9L5q4D/\nBg5r9f7DSBL7GUBt3v+OPbU9uWrIOmN1wfyONpYHpPNjSX71AxARTSS/Csel21ZE+i2RWlowPwn4\nVFrFsFHSRpJfr2M7Ck7SSZLuT6tUNpH8+h2Rbp5A8kXW2giSX+dtbSvGslYxHJ5WkaxKq4v+tYgY\nIPkSnSZpCsmV1KaIeLytgh3so9mqgvnttDw3hTEX/u070uK8pvPN5+Vqkqu+30taLOlygIhYRJKo\nrgTWSLpVUofn0rqWE4Fl4WWSL3QA0vrpCSRXBSuBcc111qmJBfPLgP8XEUMKpn4RcUsR+/05MBuY\nEBGDSaoqmvezDDi0jfesBXa2s20b0K/gOKpJqkQKte6+9zqSqpepETEI+HyrGA5pK/CI2Enyy/09\nwHuBm9sqV8Q+OrKS5Fw0m9hewTa0OK/pe18GiIgtEfGpiDgEOBf4ZHNbQET8PCJem743gK93Yp/W\nBZwILAu3A2+RdHra2PkpYBdJFdCfgAbgEkm1kv4WOLHgvf8JfDj9dS9J/dNG4IFF7HcgsD4idko6\nEXhXwbafAWdIeoekGknDJR2XXq3cBHxD0ti0MfWUtBH0L0CfdP+1wBdI2g46imEzsFXSkcBHCrbd\nAYyRdFna8DpQ0kkF238CvJ/ki3R/iWB/++jI7SR/+/FKGvcv78R7bwG+IKlO0gjgi8BPYW8D/2Fp\ngt8ENAJNko6QdFr699xJcuXY1Il9WhdwIrCSi4iFJL9sv0Pyi/sc4JyI2B0Ru4G/JfnCW09SN/6r\ngvfOAT4EfBfYQFLd8P4id/2PwFWStpB8Sd1e8LkvAWeRJKX1wDPAsenmTwN/Bp5It30dqIqITeln\n3khyNbMNaHEXURs+TZKAtpAktdsKYthCUu1zDknVzQvAGwq2P0LyJflUROyvyqbdfRThP4G7gGdJ\nGvt/tf/iLXwFmAPMJfl7PZWuA5gK3ANsJUn234uI+0kS59dI/h2sImlovqIT+7QuoJZVtWaWJ0n3\nAT+PiBvzjsUqhxOBWZmQ9CrgbpI2ji15x2OVw1VDZmVA0o9JqlYucxKwruYrAjOzCucrAjOzCtft\nOskaMWJETJ48Oe8wzMy6lSeffHJtRLR+Dgboholg8uTJzJkzJ+8wzMy6FUnt3pLsqiEzswrnRGBm\nVuGcCMzMKlxmiUDSTZLWSHqune2S9G1Ji5QMdnJ8VrGYmVn7srwi+BFw5n62v5mkf5KpwMUkPSqa\nmVkXyywRRMRDJB14tec84CeReBQYImlMVvGYmVnb8mwjGEfLATKWp+v2IeliSXMkzamvr++S4MzM\nKkW3eI4gIm4AbgCYNWuW+8Toge6Zv5q5KzYxcVg/Xlq3Le9wzMrS6UeN4tgJQ0r+uXkmghW0HClp\nfLrOUs+v2syKDTvyDiNzEXDJrU+zfXfj3nUqdrwtswoyclCfHpcIZgMfk3QrcBLJGK0rc4wnV7sb\nmti8c0+L5fOvfYSdeypnMKd+varZvruRL7zlKP7hdW2O6GhmGcgsEUi6BTgVGCFpOfAloBYgIr4P\n3EkyYtQiksG1L8oqlu7g7df/iWeXbdxn/VfOP5pjxg/OIaKu1bummgnD+vLi2m0cMaqYUSnNrFQy\nSwQRcWEH2wP4aFb7707+5X/m8eyyjZx9zBhOmjJs7/r+vWs4/7hxVFVVTj3J9LE9P+mZlZtu0Vjc\nU/1x0Vp+9thL3Pf8GvrUVvHFs6cxclCfvMMyswrjRNCF9jQ2cevjL+1tFP310yt4af12Jg3vx5fP\nP9pJwMxy4UTQRV5YvYW7F6zm3363sMX6S06fyiffeHhOUZmZORGURGNTsLuh/bt71m7dxZnf+gON\nTcGgPjU8fPlp1KT1/v16+RSYWb78LXSQIoKzv/MwC1Zu7rDs1W87hpOmDGdQn9ouiMzMrDhOBAfp\npfXbWbByM+ccO5bpYwe1W25Y/1687YTxyE9KmVmZcSI4CEvWbuO9Nz0GwKWnH8ZhI33/u5l1Px6Y\n5iDc/OhSlq3fweumjuDQugF5h2NmdkB8RdAJTU3B86u20NCUNAw//MJaTjlkODd/8KScIzMzO3BO\nBJ3wiyeX89lfzm2x7tNv8q2fZta9ORF0woMv1DNqUG/+9a0zAKiqEqccMjznqMzMDo4TQQdu/MNi\n7pibdIr6/KrNnHX0GE4/alTOUZmZlY4bi/fj8RfX85X/XUD9ll0M6lvLyYcM590nT8o7LDOzkvIV\nQTueW7GJ9/wguTX0qvOm+yrAzHosXxG040M/mcPuhiY+ccbhTgJm1qM5EbRh555GVm7ayTtmjefj\npx2WdzhmZply1VArX/3tAm7+01IAXn3oiIoaFMbMKpMTQYHlG7Zz/YOLOWrMIE47so43HDky75DM\nzDLnRFDgf55NbhP93JlHcOoRTgJmVhncRpCKCG55/CWmjhzgJGBmFcWJIPXHv67jpfXbOWHS0LxD\nMTPrUk4EqUcWrQXgijcflXMkZmZdy4kg9afF6zhh0lAG9/PoYWZWWZwIgG27Gpi7fBMnHzIs71DM\nzLqcEwHwwpqtNDYFx44fkncoZmZdzokAuHfBagAmDOuXcyRmZl2v4hNB/ZZdfOe+RQBMdCIwswpU\n8Yng0cXrAPj2hTPp39vP15lZ5XEiWLyOgb1rOOvo0XmHYmaWi4pPBC+s2cqRYwZSU13xfwozq1AV\n/e23u6GJx19cz4Shbhsws8qVaSKQdKakhZIWSbq8je0TJd0v6WlJcyWdlWU8rT30l3oAxg3t25W7\nNTMrK5klAknVwLXAm4FpwIWSprUq9gXg9oiYCVwAfC+reNry0vrtAFz0milduVszs7KS5RXBicCi\niFgcEbuBW4HzWpUJYFA6Pxh4OcN49vHS+u3071XNUHcrYWYVLMtEMA5YVrC8PF1X6ErgPZKWA3cC\nH2/rgyRdLGmOpDn19fUlCW7j9t386I9LmDF+MJJHITOzypV3Y/GFwI8iYjxwFnCzpH1iiogbImJW\nRMyqq6sryY4fWJgklLOPGVuSzzMz666yTAQrgAkFy+PTdYU+CNwOEBF/AvoAIzKMaa8fPvIig/rU\ncOGJE7tid2ZmZSvLRPAEMFXSFEm9SBqDZ7cq8xJwOoCko0gSQWnqfvZj0ZotPLt8E0ePG0y1B6c3\nswqXWSKIiAbgY8BdwAKSu4PmSbpK0rlpsU8BH5L0LHAL8P6IiKxiavbEkg0AfPn8o7PelZlZ2cu0\nc52IuJOkEbhw3RcL5ucDr8kyhrYsXbedXtVVTBnev6t3bWZWdvJuLM7Fsg3bGTe0L1WuFjIzq8xE\nsHzDDsb7aWIzM6BCE8GKDdudCMzMUhWXCLbvbmDt1t2Md0dzZmZABSaCFRt2APiKwMwsVXGJYLkT\ngZlZCxWXCNZu3QVA3YA+OUdiZlYeKi4RbNqxB4DBfd3jqJkZVGAi2LxjDxIM7OOB6s3MoAITwaYd\nexjUp9YPk5mZpSouEWzcscfVQmZmBSouEWxyIjAza6FTiUBSlaRBHZcsX04EZmYtdZgIJP1c0iBJ\n/YHngPmSPpN9aNlwIjAza6mYK4JpEbEZOB/4LTAFeG+mUWVh7u1w4xls3b6TwR6s3sxsr2ISQa2k\nWpJEMDsi9gCZDx5Tco27YfkTDN6x3FcEZmYFikkE1wNLgP7AQ5ImAZuzDCoTo48B4EhedCIwMyvQ\n4VNVEfFt4NsFq5ZKekN2IWWk7kiiuhfTq5Y4EZiZFSimsfjStLFYkn4g6SngtC6IrbRqerFz6BFM\nlxOBmVmhYqqGPpA2Fr8JGErSUPy1TKPKyOYhRyVXBO5ewsxsr2ISQXNfDGcBN0fEvIJ13cq6AUcw\nTFsZ3rQu71DMzMpGMYngSUm/J0kEd0kaCDRlG1Y2VvWdCsDwrQtzjsTMrHwUkwg+CFwOvCoitgO9\ngIsyjSojL9VOAWDgpudzjsTMrHwUc9dQk6TxwLskATwYEf+TeWQZ+NkzGzi1aRST6uflHYqZWdko\n5q6hrwGXAvPT6RJJ/5p1YFnYvruRhUxCq5/LOxQzs7JRzO0zZwHHRUQTgKQfA08Dn88ysCzsamii\naswxsPoHsGsL9B6Yd0hmZrkrtvfRIQXzg7MIpCvs2tPI2gGHAwGr5+cdjplZWSjmiuCrwNOS7ie5\nbfT1JI3H3c6OPY1sGHRksrD6zzDxpHwDMjMrA8U0Ft8i6QHgVemqz0XEqkyjykBDYxMNTUFD/zHQ\nZwis+nPeIZmZlYV2E4Gk41utWp6+jpU0NiKeyi6s0tvZkDz60KdXNYye4URgZpba3xXBf+xnW9DN\n+hvauacRgD611UlPpHNugqZGqKrOOTIzs3y1mwgiovv1MLofexNBTXpF0LAD1v0V6g7POTIzs3xl\nOni9pDMlLZS0SFKbDcyS3iFpvqR5kn6eVSy70qqh3rVVMProZOWquVntzsys28gsEUiqBq4F3gxM\nAy6UNK1VmanAFcBrImI6cFlW8TQ1JYOqVVcJRhwBVbXgB8vMzDK9IjgRWBQRiyNiN3ArcF6rMh8C\nro2IDQARsSarYBojSQRVEtT0gpFHwspns9qdmVm30Zm7hloo4q6hccCyguXlQOsb9w9P9/UIUA1c\nGRG/ayOWi4GLASZOnNjBbtvWlPaXWqW0B+2xM2HBHRAB6pa9apuZlUQxdw31AWYBz5I8UHYMMAc4\npUT7nwqcCownGRN5RkRsLCwUETcANwDMmjUrDmRHTXuvCNIVY2fCUz+BjUth6OQDCt7MrCdot2oo\nIt6Q3jm0Ejg+ImZFxAnATGBFEZ+9AphQsDy+jfctB2ZHxJ6IeBH4C0liKLmmwqohSBIBwMtPZ7E7\nM7Nuo5g2giMiYu/TVxHxHHBUEe97ApgqaYqkXsAFwOxWZX5DcjWApBEkVUWLi/jsTkvbipPGYoCR\n06G6lxOBmVW8YvoamivpRuCn6fK7gQ7vu4yIBkkfA+4iqf+/KSLmSboKmBMRs9Ntb5I0H2gEPhMR\nmYwj2Zhmgr3NATW9YNTRsKJbPSBtZlZyxSSCi4CPkIxJAPAQcF0xHx4RdwJ3tlr3xYL5AD6ZTpmK\nKLh9tNm442Hu7UlLclWmj1SYmZWtDr/9ImJnRFwTEW9Np2siYmdXBFdKzVVDVYV3CI2dCbs2w/q/\n5hOUmVkZKGaEstdIulvSXyQtbp66IrhS2qdqCNxgbGZGcVVDPwA+ATxJUo/fLe2tGirMBCOOgNp+\nSSI45h05RWZmlq9iEsGmiPht5pFkbO+TxYVtBNU1SU+kbjA2swpWTAvp/ZKulnSKpOObp8wjK7E2\n2wggqR5aNRcaG7o+KDOzMlDMFUFztxCzCtZ1u/EI9nmyuNm44+Gx62DtQhg1vesDMzPLWTFDVfaI\ncQmaex9t84oAknYCJwIzq0DFXBEg6S3AdJJ+hwCIiKuyCioL+zxZ3GzYodBrYJIIZr6n6wMzM8tZ\nMbePfh94J/Bxkk7n3g5Myjiukmvz9lFIHiQbe5xvITWzilVMY/GrI+LvgQ0R8S8kvY52u/Ed23yy\nuNnY42DVc9Cwu4ujMjPLXzGJYEf6ul3SWGAPMCa7kLLR7l1DkLQTNO6C+gVdG5SZWRkoJhHcIWkI\ncDXwFLAEyGxs4aw0tnfXEPgJYzOraMX0NfTliNgYEb8kaRs4srDjuO4iWo9HUGjoFOgzxInAzCpS\nUXcNNYuIXcCujGLJVGN7t49C0oI8dqafMDazilQxfS/vt40AkkSwZj7s6XYdq5qZHZQKSgTNfQ21\nU2DsTGhqgNXzui4oM7MyUMxzBL+S9BZJ3TpptPtkcbO9DcauHjKzylLMl/v3gHcBL0j6mqQjMo4p\nE+0+Wdxs8HjoNwJefqbrgjIzKwPF3DV0T0S8Gzie5NbReyT9UdJFkmqzDrBUmm8fbe+CYG+Dse8c\nMrMKU1R1j6ThwPuBfwCeBr5FkhjuziyyEtvv7aPNxs5MHirbva2LojIzy18xbQS/Bv4A9APOiYhz\nI+K2iPg4MCDrAEuluY2guqNEEE2w6s9dFJWZWf6KeY7g2xFxf1sbImJWW+vLUWNHt49CMjYBJNVD\nE0/OPigzszJQTNXQtLSLCQAkDZX0jxnGlIno6PZRgIGjYcAoWDm3a4IyMysDxSSCD0XExuaFiNgA\nfCi7kLLRVEwbAcDoGa4aMrOKUkwiqJZe+faUVA30yi6kbAzqU8uUEf3bv3202egZUP+8u6Q2s4pR\nTBvB74DbJF2fLv/fdF23csGJE7ngxIkdFxw9A5r2JMlgzDHZB2ZmlrNiEsHnSL78P5Iu3w3cmFlE\neRudfvmvmutEYGYVoZjB65uA69Kp5xt2CNT2dzuBmVWMDhOBpKnAV4FptBy8/pAM48pPVTWMmu5E\nYGYVo5jG4h+SXA00AG8AfgL8NMugctd851B6p5GZWU9WTCLoGxH3AoqIpRFxJfCWbMPK2egZsGsz\nbFyadyRmZpkrprF4V9oF9QuSPgasoBt1LXFAmhuJV86FoZNzDcXMLGvFXBFcStLP0CXACcB7gPcV\n8+GSzpS0UNIiSZfvp9zfSQpJ5dFlxchpoCq3E5hZRdjvFUH68Ng7I+LTwFbgomI/OH3vtcAbgeXA\nE5JmR8T8VuUGkiSbxzoZe3Zq+8KIw50IzKwi7PeKICIagdce4GefCCyKiMURsRu4FTivjXJfBr4O\nlNdgwe5qwswqRDFVQ09Lmi3pvZL+tnkq4n3jgGUFy8vTdXtJOh6YEBH/u78PknSxpDmS5tTX1xex\n6xIYPQM2L4ft67tmf2ZmOSkmEfQB1gGnAeek09kHu+O0AfobwKc6KhsRN0TErIiYVVdXd7C7Lk7h\nE8ZmZj1YMU8WF90u0MoKYELB8vh0XbOBwNHAA2mfdqOB2ZLOjYg5B7jP0hk9I3ldORcOOTXPSMzM\nMlXMk8U/BPZ5sioiPtDBW58ApkqaQpIALgDeVfD+TcCIgv08AHy6LJIAQP8RMGA0rJnfcVkzs26s\nmOcI7iiY7wO8FXi5ozdFREP63MFdQDVwU0TMk3QVMCciZh9IwF1q1DRYPS/vKMzMMlVM1dAvC5cl\n3QI8XMyHR8SdwJ2t1n2xnbKnFvOZXWrkNHj8P6GxAaqLyZlmZt1PMY3FrU0FRpY6kLI0ajo07oL1\ni/OOxMwsM8W0EWyhZRvBKpIxCnq+kdOS1zXzoO7wfGMxM8tIMVVDA7sikLJUd0TS1cTq+TD9rXlH\nY2aWiQ6rhiS9VdLgguUhks7PNqwyUdsXhh3qO4fMrEcrpo3gS+mtngBExEbgS9mFVGZ855CZ9XDF\nJIK2ylTOLTQjp8OGJbB7W96RmJllophEMEfSNyQdmk7fAJ7MOrCyMfIoIKD++bwjMTPLRDGJ4OPA\nbuA2kh5EdwIfzTKosjJqevK62u0EZtYzFXPX0Dag3UFleryhk6GmrxuMzazHKuauobslDSlYHirp\nrmzDKiNV1TDySDcYm1mPVUzV0Ij0TiEAImIDlfJkcbOR031FYGY9VjGJoEnSxOYFSZNoozfSHm3U\nNNhWD1u7aFAcM7MuVMxtoP8EPCzpQUDA64CLM42q3BR2NTHg1FxDMTMrtWIai3+XDil5crrqsohY\nm21YZab5zqE1CzxIjZn1OMU+GNYIrCEZj2CaJCLioezCKjMDRkK/EW4wNrMeqZjeR/8BuJRkqMln\nSK4M/kQyhnHlGHmUG4zNrEcqprH4UuBVwNKIeAMwE9i4/7f0QKOmw5rnoakp70jMzEqqmESwMyJ2\nAkjqHRHPA0dkG1YZGjkN9myDjUvyjsTMrKSKaSNYnj5Q9hvgbkkbgKXZhlWGCruaGHZIvrGYmZVQ\nMXcNNY/IcqWk+4HBwO8yjaoc1R0JKGknOOrsvKMxMyuZTnUnHREPZhVI2es9AIZNgVV/zjsSM7OS\nOpDB6yvX6BlOBGbW4zgRdMboGbDhRdi5Oe9IzMxKxomgM0Yfk7z6eQIz60GcCDpj1NHJq6uHzKwH\ncSLojEFjoe8wWDU370jMzErGiaAzJDcYm1mP40TQWaNnJL2QNjbkHYmZWUk4EXTW6BnQsBPWLco7\nEjOzknAi6KzRM5JXVw+ZWQ/hRNBZIw6H6l5uMDazHiPTRCDpTEkLJS2SdHkb2z8pab6kuZLuTcdD\nLm/VtcltpCueyjsSM7OSyCwRSKoGrgXeDEwDLpQ0rVWxp4FZEXEM8Avg37KKp6QmnAQrnoTGPXlH\nYmZ20LK8IjgRWBQRiyNiN3ArcF5hgYi4PyK2p4uPkoyCVv4mngQNO2Clq4fMrPvLMhGMA5YVLC9P\n17Xng8Bv29og6WJJcyTNqa+vL2GIB2jCycnrskfzjcPMrATKorFY0nuAWcDVbW2PiBsiYlZEzKqr\nq+va4NoyaAwMmQQvORGYWfeXZSJYAUwoWB6frmtB0hnAPwHnRsSuDOMprYmnwNJHPIaxmXV7WSaC\nJ4CpkqZI6gVcAMwuLCBpJnA9SRJYk2EspXfoabB9Hax6Nu9IzMwOSmaJICIagI8BdwELgNsjYp6k\nqySdmxa7GhgA/JekZyTNbufjys+hpyWvi+7JNw4zs4PUqaEqOysi7gTubLXuiwXzZ2S5/0wNqIMx\nx8Gie+H1n8k7GjOzA1YWjcXd1mFnwLLHYcfGvCMxMztgTgQH47AzIBph8f15R2JmdsCcCA7G+FdB\n/zqY95u8IzEzO2BOBAejugamnQ9/uQt2bck7GjOzA+JEcLBmvC3pbmLer/OOxMzsgDgRHKwJJ8HI\nafD4DRCRdzRmZp3mRHCwJDjx4mSgGnc5YWbdkBNBKRzzDugzGB77ft6RmJl1mhNBKfTqD7M+APP/\nG9Y8n3c0Zmad4kRQKq++BHoNgAe+mnckZmad4kRQKv2GwSn/CPN/A0sezjsaM7OiORGU0msug6GT\nYfYlsGdH3tGYmRXFiaCUevWDc74F6/8Kd3zCt5OaWbfgRFBqh5wKp14Bz94CD/173tGYmXUo026o\nK9brPwvrX4T7vwKNu+DUz0OVc66ZlScngixUVcH530v6Inroalg5F86+BgaPyzsyM7N9+GdqVqqq\n4dzvwpuvhhcfhO/OSpLCrq15R2Zm1oITQZYkOOli+OjjydgF930FrpkGv/8CrHjSA9+bWVlQdLM7\nW2bNmhVz5szJO4wDs+wJePRamD87GdBmwCg49HSY8jqY/FoYMjHvCM2sh5L0ZETMamub2wi60oRX\nwYQfwfb18MLdsPBO+Mvv4NmfJ9uHTIKJJ8O4E2Ds8TB6BtT2yTVkM+v5nAjy0G8YHPvOZGpqgvoF\n8OIfYMkfYPEDMPe2pFxVDYyaniSG5uRQd0TS/mBmViKuGio3EbD5ZXj5qaQdYcWT8PIzsGtzsr22\nH9QdCaOmwcjpr7wOqMs3bjMra64a6k6k5DbTwePgqHOSdU1NsG5RkhRWPgOr58HC38LTP33lff3r\nkgFyRh4FQ6ckXV0MnZy0O/Tql8eRmFk34UTQHVRVQd3hyXTchcm6CNi6BtbMT6bV82HNPHjqZtiz\nreX7B4x+JTEMnVQwPznZ5ofdzCqaE0F3JcHAUcl06BteWR8B29fBhiXp9GL6uhSWPpK2PxRUB1b3\nfiU5DB4PA8fAwNEwcGz6OiZp05C69PDMrOs4EfQ0EvQfkUzj26gObNgNm5YVJIglSZLY8GJS9bR9\n3b7vqe6VXDkMGJlM/euSqXl+wEjoPzLZZ9+hThpm3YwTQaWp6QXDD02mtjTsgi2r0mnlK9PmlbBt\nTZI0lj+RJIxo44G4qto0OaTJov/IdH5kwfo0ofQb7jugzMqAE4G1VNNcVTRp/+WaGpPnIbatSdoq\ntq0tmK9Ppq1rYM2C5LVpTxsfoiQZtLiySBNI3yHQe1AyFnTvQdCnYL5Xf191mJWQE4EdmKrq5Nf9\ngLrkWYf9iYCdm15JDtvSxLCdOmQAAAhVSURBVNF6fvkTsLV+38buttT2g9q+UNM3ea3t+8q6va/t\nrKvpDTV9kiqvmt7pa59W872S9pOadKrunXQiaNYD+V+2ZU9KfuH3HQIjpnZcfvf2JHHs2py87twM\nuza9Mr97GzTsSEaB27MD9mx/5XX31iTh7F2XTo27SnAcVa0SSGGiSBNIdW3yIGBVdfKqqlbL1el8\n4XJNcudWi+W0TIvlnD/PeiwnAis/vfqlzz6MKd1nNjW+kiwadiVTY/PrbmjYmTSkN6/buz3d1rg7\nXV8w3+b7dib7ampI+pNqauxguSF5TqRwua22l3JQmChUnSSHFsv7W1/VRrm23tte2WL3VdPJuDpY\nf1DH3M76qpqyq9p0IrDKUFUNvQckU7mLSJLF3kRRkCBaLBeZWJrLHOznFZaJpoLPbH5tarW8v/Xp\n+xt2FV+2mH11GzqwZHLq5XD035U8mkwTgaQzgW8B1cCNEfG1Vtt7Az8BTgDWAe+MiCVZxmRW9qS0\nPaIG6J13NN1HRDtJo71kUkSS6TD5HcBnt/jMTia7vkMz+dNllggkVQPXAm8ElgNPSJodEfMLin0Q\n2BARh0m6APg68M6sYjKzHkx65ZezdUqWLUAnAosiYnFE7AZuBc5rVeY84Mfp/C+A06UyqzwzM+vh\nskwE44BlBcvL03VtlomIBmATMLz1B0m6WNIcSXPq6+szCtfMrDJ1i3vCIuKGiJgVEbPq6tzdsplZ\nKWWZCFYAEwqWx6fr2iwjqQYYTNJobGZmXSTLRPAEMFXSFEm9gAuA2a3KzAbel86/DbgvuttIOWZm\n3Vxmdw1FRIOkjwF3kdw+elNEzJN0FTAnImYDPwBulrQIWE+SLMzMrAtl+hxBRNwJ3Nlq3RcL5ncC\nb88yBjMz279u0VhsZmbZ6XaD10uqB5Ye4NtHAGtLGE534GOuDD7mynAwxzwpItq87bLbJYKDIWlO\nRLQxbFfP5WOuDD7mypDVMbtqyMyswjkRmJlVuEpLBDfkHUAOfMyVwcdcGTI55opqIzAzs31V2hWB\nmZm14kRgZlbhKiYRSDpT0kJJiyRdnnc8pSJpgqT7Jc2XNE/Spen6YZLulvRC+jo0XS9J307/DnMl\nHZ/vERwYSdWSnpZ0R7o8RdJj6XHdlvZvhaTe6fKidPvkPOM+UJKGSPqFpOclLZB0SgWc40+k/6af\nk3SLpD498TxLuknSGknPFazr9LmV9L60/AuS3tfWvtpTEYmgYLS0NwPTgAslTcs3qpJpAD4VEdOA\nk4GPpsd2OXBvREwF7k2XIfkbTE2ni4Hruj7kkrgUWFCw/HXgmog4DNhAMvodFIyCB1yTluuOvgX8\nLiKOBI4lOfYee44ljQMuAWZFxNEk/ZU1j2LY087zj4AzW63r1LmVNAz4EnASyaBgX2pOHkWJiB4/\nAacAdxUsXwFckXdcGR3rf5MMD7oQGJOuGwMsTOevBy4sKL+3XHeZSLo0vxc4DbgDEMnTljWtzzdJ\np4enpPM1aTnlfQydPN7BwIut4+7h57h50Kph6Xm7A/ibnnqegcnAcwd6boELgesL1rco19FUEVcE\nFDdaWreXXg7PBB4DRkXEynTTKmBUOt8T/hbfBD4LNKXLw4GNkYxyBy2PqahR8MrcFKAe+GFaHXaj\npP704HMcESuAfwdeAlaSnLcn6dnnuVBnz+1BnfNKSQQ9nqQBwC+ByyJic+G2SH4i9Ij7hCWdDayJ\niCfzjqUL1QDHA9dFxExgG69UFQA96xwDpNUa55EkwbFAf/atPqkIXXFuKyURFDNaWrclqZYkCfws\nIn6Vrl4taUy6fQywJl3f3f8WrwHOlbQEuJWkeuhbwJB0lDtoeUw9YRS85cDyiHgsXf4FSWLoqecY\n4AzgxYioj4g9wK9Izn1PPs+FOntuD+qcV0oiKGa0tG5JkkgG+FkQEd8o2FQ4+tv7SNoOmtf/fXr3\nwcnApoJL0LIXEVdExPiImExyHu+LiHcD95OMcgf7Hm+3HgUvIlYByyQdka46HZhPDz3HqZeAkyX1\nS/+NNx9zjz3PrXT23N4FvEnS0PRq6k3puuLk3UjShY0xZwF/Af4K/FPe8ZTwuF5Lctk4F3gmnc4i\nqR+9F3gBuAcYlpYXyR1UfwX+THJXRu7HcYDHfipwRzp/CPA4sAj4L6B3ur5Purwo3X5I3nEf4LEe\nB8xJz/NvgKE9/RwD/wI8DzwH3Az07onnGbiFpB1kD8nV3wcP5NwCH0iPfxFwUWdicBcTZmYVrlKq\nhszMrB1OBGZmFc6JwMyswjkRmJlVOCcCM7MK50Rg1oUkndrcY6pZuXAiMDOrcE4EZm2Q9B5Jj0t6\nRtL16fgHWyVdk/aRf6+kurTscZIeTfuH/3VB3/GHSbpH0rOSnpJ0aPrxAwrGFvhZ+uSsWW6cCMxa\nkXQU8E7gNRFxHNAIvJuk47M5ETEdeJCk/3eAnwCfi4hjSJ72bF7/M+DaiDgWeDXJ06OQ9BB7GcnY\nGIeQ9KFjlpuajouYVZzTgROAJ9If631JOv1qAm5Ly/wU+JWkwcCQiHgwXf9j4L8kDQTGRcSvASJi\nJ0D6eY9HxPJ0+RmSvugfzv6wzNrmRGC2LwE/jogrWqyU/rlVuQPtn2VXwXwj/n9oOXPVkNm+7gXe\nJmkk7B0/dhLJ/5fmni/fBTwcEZuADZJel65/L/BgRGwBlks6P/2M3pL6delRmBXJv0TMWomI+ZK+\nAPxeUhVJr5AfJRkQ5sR02xqSdgRIugn+fvpFvxi4KF3/XuB6SVeln/H2LjwMs6K591GzIknaGhED\n8o7DrNRcNWRmVuF8RWBmVuF8RWBmVuGcCMzMKpwTgZlZhXMiMDOrcE4EZmYV7v8DhWBgqgf3sKkA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHSj8iKrtLjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b59356e4-bb7a-49c0-9634-0203c9ff4ded"
      },
      "source": [
        "print('shape of training data:', X_train.shape)\n",
        "print('shape of test data:', X_test.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of training data: (168, 8)\n",
            "shape of test data: (43, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzjQdyxqtk_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "cf8ffb72-2042-487c-abcb-b90569f84aef"
      },
      "source": [
        "#defining threshold as 0.5\n",
        "pred = model.predict(X_test)\n",
        "pred = [1 if y>=0.5 else 0 for y in pred]\n",
        "pred"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvPLokk6tqGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "877f0793-4d3c-467f-cd66-c2c10ad77e7e"
      },
      "source": [
        "#Show the actual values\n",
        "Y_test"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "270    1\n",
              "252    1\n",
              "366    1\n",
              "387    1\n",
              "329    1\n",
              "3      0\n",
              "277    1\n",
              "320    1\n",
              "386    1\n",
              "289    1\n",
              "92     0\n",
              "20     0\n",
              "379    1\n",
              "299    1\n",
              "80     0\n",
              "281    1\n",
              "353    1\n",
              "352    1\n",
              "18     0\n",
              "246    0\n",
              "24     0\n",
              "48     0\n",
              "325    1\n",
              "176    0\n",
              "54     0\n",
              "258    1\n",
              "377    1\n",
              "354    1\n",
              "94     0\n",
              "196    0\n",
              "189    0\n",
              "190    0\n",
              "364    1\n",
              "337    1\n",
              "133    0\n",
              "278    1\n",
              "108    0\n",
              "308    1\n",
              "359    1\n",
              "313    1\n",
              "259    1\n",
              "397    1\n",
              "385    1\n",
              "Name: classification, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRI1tRdetuAQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f50ccdb1-5b1b-4ded-d5ab-6855c3d01013"
      },
      "source": [
        "#Show the actual and predicted values\n",
        "print('Original : {0}'.format(\", \".join(str(x) for x in Y_test)))\n",
        "print('Predicted : {0}'.format(\", \".join(str(x) for x in pred)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original : 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1\n",
            "Predicted : 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYnB_PnSuAtw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53696806-d693-4802-dadb-0081fc66015d"
      },
      "source": [
        "n = (Y_test == 1).sum()\n",
        "n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYtrCSjunsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eed81cfc-a911-4911-88da-e7cc5398a2b8"
      },
      "source": [
        "m = (Y_test == 0).sum() + n\n",
        "m"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVYdQqoWuxTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "966a1cba-7695-426a-d81a-8cd55cbda4ca"
      },
      "source": [
        "factor=(n*100)/m\n",
        "factor"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62.7906976744186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp7wor_DvG5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b0bf9a6-3eda-4e38-884e-188e8dc8764b"
      },
      "source": [
        "if factor <= 50:\n",
        " print(\"Below Average Liveability Factor\");\n",
        "elif factor>50 and factor<70:\n",
        " print(\"Average Liveability Factor\");\n",
        "elif factor>=70 and factor<80:\n",
        " print(\"Good Liveability Factor\");\n",
        "else:\n",
        " print(\"Excellent Liveability Factor\");"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Liveability Factor\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}